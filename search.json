[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "kaggle_geopandas(1) - Your First Map\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nkaggle geopandas(2) - Coordinate Reference Systems (좌표참조 시스템)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nkaggle geopandas(3) - Interactive Maps\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNumpy Code!\n\n\n\n\n\n\n\ncoding\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\npandas Code! (1)\n\n\n\n\n\n\n\ncoding\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nseaborn_and_matplotlib\n\n\n\n\n\n\n\ncoding\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFor loop 속도 개선하기\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/kaggle_gopandas1.html",
    "href": "posts/kaggle_gopandas1.html",
    "title": "kaggle_geopandas(1) - Your First Map",
    "section": "",
    "text": "GeoPandas에서 에서 플로팅을 시작하세요."
  },
  {
    "objectID": "posts/kaggle_gopandas1.html#introduction",
    "href": "posts/kaggle_gopandas1.html#introduction",
    "title": "kaggle_geopandas(1) - Your First Map",
    "section": "Introduction",
    "text": "Introduction\nmicro-course에서 지리 공간 데이터 또는 지리적 위치가 있는 데이터를 랭글링하고 시각화하는 다양한 방법에 대해 알아봅니다.\n그 과정에서 다음과 같은 몇 가지 실제 문제에 대한 솔루션을 제공하게 됩니다: - 글로벌 비영리 단체가 필리핀의 외딴 지역에서 활동 범위를 넓히려면 어디로 가야 할까요? - 멸종 위기 조류인 보라색 담비는 북미와 남미를 어떻게 이동하나요? - 새들이 보호 지역으로 이동하는가? - 일본의 어느 지역이 추가 내진 보강의 혜택을 받을 수 있을까요? - 캘리포니아의 어느 스타벅스 매장이 다음 스타벅스 리저브 로스터리 매장으로 유력한 후보지인가요? - 뉴욕시에는 자동차 충돌 사고에 대응할 수 있는 충분한 병원이 있나요? 뉴욕시에서 의료 서비스 제공에 공백이 있는 지역은 어디일까요?\n또한 보스턴시의 범죄를 시각화하고, 가나의 의료 시설을 조사하고, 유럽의 최고 대학을 탐색하고, 미국의 독성 화학물질 방출을 추적할 수 있습니다.\n이 첫 번째 튜토리얼에서는 이 마이크로 강좌를 완료하는 데 필요한 전제 조건을 빠르게 다룹니다. 더 깊이 있는 복습을 원하신다면 판다스 마이크로 코스를 추천합니다.\n또한 첫 번째 지리공간 데이터 집합을 시각화해 보겠습니다."
  },
  {
    "objectID": "posts/kaggle_gopandas1.html#reading-data",
    "href": "posts/kaggle_gopandas1.html#reading-data",
    "title": "kaggle_geopandas(1) - Your First Map",
    "section": "Reading data",
    "text": "Reading data\n첫 번째 단계는 지리 공간 데이터를 읽어오는 것입니다! 이를 위해 GeoPandas 라이브러리를 사용하겠습니다.\n\nimport geopandas as gpd\n\nC:\\Users\\heeyoung\\.conda\\envs\\quarto\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.1-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n  warnings.warn(\n\n\nshapefile, GeoJSON, KML, GPKG 등 다양한 지리공간 파일 형식이 있습니다. 이 마이크로 강좌에서는 그 차이점에 대해서는 다루지 않겠지만, 중요합니다:\n\nshapefile은 가장 흔한 파일 유형입니다.\n이러한 모든 파일 유형은 gpd.read_file() 함수를 사용하여 빠르게 로드할 수 있습니다. 다음 코드 셀은 뉴욕주 환경보전국에서 관리하는 숲, 야생지대 및 기타 토지에 대한 정보가 포함된 shapefile을 로드합니다.\n\n\n# Read in the data\nfull_data = gpd.read_file(\"geopandas_data/DEC_lands/DEC_lands/DEC_lands.shp\")\n\n# View the first five rows of the data\nfull_data.head()\n\n\n\n\n\n\n\n\nOBJECTID\nCATEGORY\nUNIT\nFACILITY\nCLASS\nUMP\nDESCRIPTIO\nREGION\nCOUNTY\nURL\nSOURCE\nUPDATE_\nOFFICE\nACRES\nLANDS_UID\nGREENCERT\nSHAPE_AREA\nSHAPE_LEN\ngeometry\n\n\n\n\n0\n1\nFOR PRES DET PAR\nCFP\nHANCOCK FP DETACHED PARCEL\nWILD FOREST\nNone\nDELAWARE COUNTY DETACHED PARCEL\n4\nDELAWARE\nhttp://www.dec.ny.gov/\nDELAWARE RPP\n5/12\nSTAMFORD\n738.620192\n103\nN\n2.990365e+06\n7927.662385\nPOLYGON ((486093.245 4635308.586, 486787.235 4...\n\n\n1\n2\nFOR PRES DET PAR\nCFP\nHANCOCK FP DETACHED PARCEL\nWILD FOREST\nNone\nDELAWARE COUNTY DETACHED PARCEL\n4\nDELAWARE\nhttp://www.dec.ny.gov/\nDELAWARE RPP\n5/12\nSTAMFORD\n282.553140\n1218\nN\n1.143940e+06\n4776.375600\nPOLYGON ((491931.514 4637416.256, 491305.424 4...\n\n\n2\n3\nFOR PRES DET PAR\nCFP\nHANCOCK FP DETACHED PARCEL\nWILD FOREST\nNone\nDELAWARE COUNTY DETACHED PARCEL\n4\nDELAWARE\nhttp://www.dec.ny.gov/\nDELAWARE RPP\n5/12\nSTAMFORD\n234.291262\n1780\nN\n9.485476e+05\n5783.070364\nPOLYGON ((486000.287 4635834.453, 485007.550 4...\n\n\n3\n4\nFOR PRES DET PAR\nCFP\nGREENE COUNTY FP DETACHED PARCEL\nWILD FOREST\nNone\nNone\n4\nGREENE\nhttp://www.dec.ny.gov/\nGREENE RPP\n5/12\nSTAMFORD\n450.106464\n2060\nN\n1.822293e+06\n7021.644833\nPOLYGON ((541716.775 4675243.268, 541217.579 4...\n\n\n4\n6\nFOREST PRESERVE\nAFP\nSARANAC LAKES WILD FOREST\nWILD FOREST\nSARANAC LAKES\nNone\n5\nESSEX\nhttp://www.dec.ny.gov/lands/22593.html\nDECRP, ESSEX RPP\n12/96\nRAY BROOK\n69.702387\n1517\nN\n2.821959e+05\n2663.909932\nPOLYGON ((583896.043 4909643.187, 583891.200 4...\n\n\n\n\n\n\n\n“CLASS” 열에서 볼 수 있듯이 처음 5개의 행은 각각 다른 forest에 해당합니다.\n이 튜토리얼의 나머지 부분에서는 이 데이터를 사용하여 주말 캠핑 여행을 계획하는 시나리오를 생각해 보겠습니다. 온라인에서 크라우드 소싱된 리뷰에 의존하는 대신 자신만의 지도를 만들기로 결정합니다. 이렇게 하면 특정 관심사에 맞게 여행을 조정할 수 있습니다."
  },
  {
    "objectID": "posts/kaggle_gopandas1.html#prerequisites",
    "href": "posts/kaggle_gopandas1.html#prerequisites",
    "title": "kaggle_geopandas(1) - Your First Map",
    "section": "Prerequisites",
    "text": "Prerequisites\n데이터의 처음 다섯 행을 보기 위해 head() 메서드를 사용했습니다. 이 메서드는 판다스 데이터프레임을 미리 볼 때에도 사용한다는 것을 기억하실 것입니다. 사실, 데이터프레임에 사용할 수 있는 모든 명령은 데이터와 함께 작동합니다!\n데이터가 (판다스) 데이터프레임의 모든 기능을 갖춘 (GeoPandas) GeoDataFrame 객체에 로드되었기 때문입니다.\n\ntype(full_data)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n예를 들어 모든 열을 사용하지 않으려는 경우 열의 하위 집합을 선택할 수 있습니다. (데이터를 선택하는 다른 방법을 검토하려면 Pandas micro-course 강좌의 튜토리얼을 확인하세요.)\n\ndata = full_data.loc[:, [\"CLASS\", \"COUNTY\", \"geometry\"]].copy()\n\nvalue_counts() 메서드를 사용하여 다양한 토지 유형 목록과 해당 유형이 데이터 집합에 나타나는 횟수를 확인합니다.\n\n# How many lands of each type are there?\ndata.CLASS.value_counts()\n\nCLASS\nWILD FOREST                   965\nINTENSIVE USE                 108\nPRIMITIVE                      60\nWILDERNESS                     52\nADMINISTRATIVE                 17\nUNCLASSIFIED                    7\nHISTORIC                        5\nPRIMITIVE BICYCLE CORRIDOR      4\nCANOE AREA                      1\nName: count, dtype: int64\n\n\nloc(혹은iloc) 및 isin을 사용하여 데이터의 하위 집합을 선택할 수도 있습니다.\n\n# Select lands that fall under the \"WILD FOREST\" or \"WILDERNESS\" category\nwild_lands = data.loc[data.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()\nwild_lands.head()\n\n\n\n\n\n\n\n\nCLASS\nCOUNTY\ngeometry\n\n\n\n\n0\nWILD FOREST\nDELAWARE\nPOLYGON ((486093.245 4635308.586, 486787.235 4...\n\n\n1\nWILD FOREST\nDELAWARE\nPOLYGON ((491931.514 4637416.256, 491305.424 4...\n\n\n2\nWILD FOREST\nDELAWARE\nPOLYGON ((486000.287 4635834.453, 485007.550 4...\n\n\n3\nWILD FOREST\nGREENE\nPOLYGON ((541716.775 4675243.268, 541217.579 4...\n\n\n4\nWILD FOREST\nESSEX\nPOLYGON ((583896.043 4909643.187, 583891.200 4...\n\n\n\n\n\n\n\n위의 명령어에 익숙하지 않은 경우, 이 페이지를 북마크에 추가하여 필요에 따라 명령을 찾아볼 수 있도록 하는 것이 좋습니다. 이 micro-course에서는 맵을 만들기 전에 데이터를 이해하고 필터링하기 위해 이러한 명령을 사용할 것입니다."
  },
  {
    "objectID": "posts/kaggle_gopandas1.html#create-your-first-map",
    "href": "posts/kaggle_gopandas1.html#create-your-first-map",
    "title": "kaggle_geopandas(1) - Your First Map",
    "section": "Create your first map!",
    "text": "Create your first map!\nPlot() 메서드를 사용하여 데이터를 빠르게 시각화할 수 있습니다.\n\nwild_lands.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n모든 GeoDataFrame에는 특별한 “geometry” 열이 포함되어 있습니다. 이 열에는 plot() 메서드를 호출할 때 표시되는 모든 기하학적 개체가 포함됩니다.\n\n# View the first five entries in the \"geometry\" column\nwild_lands.geometry.head()\n\n0    POLYGON ((486093.245 4635308.586, 486787.235 4...\n1    POLYGON ((491931.514 4637416.256, 491305.424 4...\n2    POLYGON ((486000.287 4635834.453, 485007.550 4...\n3    POLYGON ((541716.775 4675243.268, 541217.579 4...\n4    POLYGON ((583896.043 4909643.187, 583891.200 4...\nName: geometry, dtype: geometry\n\n\n이 열에는 다양한 데이터 유형이 포함될 수 있지만 각 항목은 일반적으로 Point, LineString, 또는 Polygon 입니다.\n데이터 집합의 ‘geometry’ 열에는 2983개의 서로 다른 Polygon 개체가 포함되어 있으며, 각 개체는 위 플롯에서 서로 다른 모양에 해당합니다.\n아래 코드 셀에서는 캠프장 위치(Point), 도보 트레일(LineString), 카운티 경계(Polygon)를 포함하는 세 개의 GeoDataFrames를 더 만듭니다.\n\n# Campsites in New York state (Point)\nPOI_data = gpd.read_file(\"geopandas_data/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp\")\ncampsites = POI_data.loc[POI_data.ASSET=='PRIMITIVE CAMPSITE'].copy()\n\n# Foot trails in New York state (LineString)\nroads_trails = gpd.read_file(\"geopandas_data/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp\")\ntrails = roads_trails.loc[roads_trails.ASSET=='FOOT TRAIL'].copy()\n\n# County boundaries in New York state (Polygon)\ncounties = gpd.read_file(\"geopandas_data/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp\")\n\n다음으로, 네 개의 GeoDataFrames 모두에서 맵을 만듭니다.\nplot() 메서드는 모양을 사용자 지정하는 데 사용할 수 있는 몇 가지 매개 변수를 (선택 사항) 입력으로 받습니다. 가장 중요한 것은 ax 값을 설정하면 모든 정보가 동일한 맵에 그려진다는 것입니다.\n\n# Define a base map with county boundaries\nax = counties.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\n# Add wild lands, campsites, and foot trails to the base map\nwild_lands.plot(color='lightgreen', ax=ax)\ncampsites.plot(color='maroon', markersize=2, ax=ax)\ntrails.plot(color='black', markersize=1, ax=ax)\n\n&lt;Axes: &gt;\n\n\n\n\n\n주 북동부 지역은 캠핑 여행에 좋은 옵션이 될 것 같습니다!"
  },
  {
    "objectID": "posts/kaggle_gopandas1.html#your-turn",
    "href": "posts/kaggle_gopandas1.html#your-turn",
    "title": "kaggle_geopandas(1) - Your First Map",
    "section": "Your turn",
    "text": "Your turn\n처음에는 복잡하게 느껴지겠지만, 이미 중요한 분석을 수행할 수 있을 만큼 충분히 배웠을 것입니다. 비영리 단체가 사업을 확장할 수 있는 필리핀의 외딴 지역을 직접 찾아보세요."
  },
  {
    "objectID": "posts/kaggle_gopandas1.html#introduction-1",
    "href": "posts/kaggle_gopandas1.html#introduction-1",
    "title": "kaggle_geopandas(1) - Your First Map",
    "section": "introduction",
    "text": "introduction\nKiva.org는 전 세계 빈곤층에게 금융 서비스를 제공하는 온라인 크라우드펀딩 플랫폼입니다.\nKiva의 대출자들은 200만 명 이상의 사람들에게 10억 달러 이상의 대출을 제공했습니다.\nKiva는 전 세계 ‘현장 파트너’ 네트워크를 통해 전 세계에서 가장 외진 곳까지 도달합니다. 이러한 파트너는 지역사회에서 대출자를 심사하고, 서비스를 제공하며, 대출을 관리하는 현지 단체입니다.\n이 연습에서는 필리핀의 Kiva 대출을 조사하게 됩니다. 새로운 현장 파트너를 모집할 기회를 찾기 위해 현재 Kiva의 네트워크 밖에 있을 수 있는 지역을 파악할 수 있나요?\n시작하려면 아래 코드 셀을 실행하여 피드백 시스템을 설정하세요.\n\nimport geopandas as gpd\n\n\n1) Get the data.\n다음 셀을 사용하여 loans_filepath에 있는 shapefile을 로드하여 GeoDataFrame world_loans를 만듭니다.\n\nloans_filepath = \"./geopandas_data/kiva_loans/kiva_loans/kiva_loans.shp\"\n\n# Your code here: Load the data\nworld_loans = gpd.read_file(loans_filepath)\n\n# Check your answer\n#q_1.check()\n\n# Uncomment to view the first five rows of the data\nworld_loans.head()\n\n\n\n\n\n\n\n\nPartner ID\nField Part\nsector\nLoan Theme\ncountry\namount\ngeometry\n\n\n\n\n0\n9\nKREDIT Microfinance Institution\nGeneral Financial Inclusion\nHigher Education\nCambodia\n450\nPOINT (102.89751 13.66726)\n\n\n1\n9\nKREDIT Microfinance Institution\nGeneral Financial Inclusion\nVulnerable Populations\nCambodia\n20275\nPOINT (102.98962 13.02870)\n\n\n2\n9\nKREDIT Microfinance Institution\nGeneral Financial Inclusion\nHigher Education\nCambodia\n9150\nPOINT (102.98962 13.02870)\n\n\n3\n9\nKREDIT Microfinance Institution\nGeneral Financial Inclusion\nVulnerable Populations\nCambodia\n604950\nPOINT (105.31312 12.09829)\n\n\n4\n9\nKREDIT Microfinance Institution\nGeneral Financial Inclusion\nSanitation\nCambodia\n275\nPOINT (105.31312 12.09829)\n\n\n\n\n\n\n\n\n\n2) Plot the data.\n다음 코드 셀을 변경하지 않고 실행하여 국가 경계가 포함된 GeoDataFrame world를 로드합니다.\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\n\n\n\n\n0\n889953.0\nOceania\nFiji\nFJI\n5496\nMULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n\n\n1\n58005463.0\nAfrica\nTanzania\nTZA\n63177\nPOLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n\n\n2\n603253.0\nAfrica\nW. Sahara\nESH\n907\nPOLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n\n\n3\n37589262.0\nNorth America\nCanada\nCAN\n1736425\nMULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n\n\n4\n328239523.0\nNorth America\nUnited States of America\nUSA\n21433226\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n\n\n\n\n\nworld 및 world_loans 지오데이터프레임을 사용하여 전 세계의 Kiva 대출 위치를 시각화합니다.\n\nax = world.plot(figsize=(10,10), color='whitesmoke', edgecolor='gainsboro', zorder=3)\nworld_loans.plot(color='lightgreen', ax=ax)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n3) Select loans based in the Philippines.\n다음으로 필리핀에 기반을 둔 대출에 중점을 두겠습니다.\n다음 코드 셀을 사용하여 필리핀을 기반으로 하는 대출이 있는 world_loans의 모든 행을 포함하는 GeoDataFrame PHL_loans를 만듭니다.\n\nPHL_loans = world_loans.loc[world_loans.country==\"Philippines\"].copy()\n\n\n\n4) Understand loans in the Philippines.\n다음 코드 셀을 변경하지 않고 실행하여 필리핀의 모든 섬에 대한 경계가 포함된 GeoDataFrame PHL을 로드합니다.\n\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"./geopandas_data/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\n\n\n\n\n\n\n\nName\nDescription\ngeometry\n\n\n\n\n0\nAutonomous Region in Muslim Mindanao\n\nMULTIPOLYGON (((119.46690 4.58718, 119.46653 4...\n\n\n1\nBicol Region\n\nMULTIPOLYGON (((124.04577 11.57862, 124.04594 ...\n\n\n2\nCagayan Valley\n\nMULTIPOLYGON (((122.51581 17.04436, 122.51568 ...\n\n\n3\nCalabarzon\n\nMULTIPOLYGON (((120.49202 14.05403, 120.49201 ...\n\n\n4\nCaraga\n\nMULTIPOLYGON (((126.45401 8.24400, 126.45407 8...\n\n\n\n\n\n\n\n필리핀의 대출을 시각화하려면 PHL 및 PHL_loans 지오데이터프레임을 사용합니다.\n\nax = PHL.plot(figsize=(10,10), color='none', edgecolor='lightgray', zorder=3)\nPHL_loans.plot(color='gray', ax=ax)\n\n&lt;Axes: &gt;\n\n\n\n\n\n새로운 현장 파트너를 모집하는 데 도움이 될 만한 섬을 파악할 수 있나요? 현재 키바의 손이 닿지 않는 섬이 있나요?\n이 지도가 질문에 답하는 데 도움이 될 수 있습니다."
  },
  {
    "objectID": "posts/kaggle_gopandas2.html",
    "href": "posts/kaggle_gopandas2.html",
    "title": "kaggle geopandas(2) - Coordinate Reference Systems (좌표참조 시스템)",
    "section": "",
    "text": "지구 표면을 2차원으로 표현할 수 있다는 것은 정말 놀라운 일입니다!"
  },
  {
    "objectID": "posts/kaggle_gopandas2.html#introduction",
    "href": "posts/kaggle_gopandas2.html#introduction",
    "title": "kaggle geopandas(2) - Coordinate Reference Systems (좌표참조 시스템)",
    "section": "introduction",
    "text": "introduction\n이 강좌에서 만드는 지도는 지구 표면을 2차원으로 묘사합니다. 하지만 아시다시피 세계는 실제로 3차원의 지구입니다. 따라서 지도 투영이라는 방법을 사용하여 평평한 표면으로 렌더링해야 합니다.\n지도 투영은 100% 정확할 수 없습니다. 각 투영법은 지구 표면을 어떤 식으로든 왜곡하지만 유용한 속성은 유지합니다.\n예를 들어\n\n등면적 투영(“램버트 원통형 등면적” 또는 “아프리카 알버스 등면적 원뿔” 등)은 면적을 보존합니다. 예를 들어 국가나 도시의 면적을 계산하려는 경우 좋은 선택입니다.\n등거리 투영(“방위각 등거리 투영” 등)은 거리를 보존합니다. 비행 거리를 계산할 때 좋은 선택입니다.\n\n투영된 점이 지구상의 실제 위치와 어떻게 일치하는지 보여주기 위해 좌표 참조 시스템(CRS)을 사용합니다. 이 튜토리얼에서는 좌표 참조계에 대해 자세히 알아보고 geopandas에서 좌표 참조계를 사용하는 방법을 알아봅니다.\n\nimport geopandas as gpd\nimport pandas as pd\n\nC:\\Users\\heeyoung\\.conda\\envs\\quarto\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.1-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n  warnings.warn("
  },
  {
    "objectID": "posts/kaggle_gopandas2.html#setting-the-crs",
    "href": "posts/kaggle_gopandas2.html#setting-the-crs",
    "title": "kaggle geopandas(2) - Coordinate Reference Systems (좌표참조 시스템)",
    "section": "Setting the CRS",
    "text": "Setting the CRS\n셰이프 파일에서 GeoDataFrame을 만들 때 CRS는 이미 가져온 것입니다.\n\nimport geopandas as gpd\nimport pandas as pd\nimport matplotlib\n\n\n# Load a GeoDataFrame containing regions in Ghana\nregions = gpd.read_file(\"geopandas_data/ghana/ghana/Regions/Map_of_Regions_in_Ghana.shp\")\nprint(regions.crs)\n\nepsg:32630\n\n\n이를 어떻게 해석하나요?\n좌표 참조 시스템은 유럽 석유 측량 그룹(EPSG) 코드에 의해 참조됩니다.\n이 지리데이터 프레임은 일반적으로 “Mercator” 투영법이라고 더 많이 불리는 EPSG 32630을 사용합니다. 이 투영법은 각도를 보존하고(해상 항해에 유용함) 면적을 약간 왜곡합니다.\n그러나 CSV 파일에서 GeoDataFrame을 만들 때는 CRS를 설정해야 합니다. EPSG 4326은 위도 및 경도 좌표에 해당합니다.\n\n# Create a DataFrame with health facilities in Ghana\nfacilities_df = pd.read_csv(\"geopandas_data/ghana/ghana/health_facilities.csv\")\n\n# Convert the DataFrame to a GeoDataFrame\nfacilities = gpd.GeoDataFrame(facilities_df, geometry=gpd.points_from_xy(facilities_df.Longitude, facilities_df.Latitude))\n\n# Set the coordinate reference system (CRS) to EPSG 4326\nfacilities.crs = {'init': 'epsg:4326'}\n\n# View the first five rows of the GeoDataFrame\nfacilities.head()\n\nC:\\Users\\heeyoung\\.conda\\envs\\quarto\\lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n\n\n\nRegion\nDistrict\nFacilityName\nType\nTown\nOwnership\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nAshanti\nOffinso North\nA.M.E Zion Clinic\nClinic\nAfrancho\nCHAG\n7.40801\n-1.96317\nPOINT (-1.96317 7.40801)\n\n\n1\nAshanti\nBekwai Municipal\nAbenkyiman Clinic\nClinic\nAnwiankwanta\nPrivate\n6.46312\n-1.58592\nPOINT (-1.58592 6.46312)\n\n\n2\nAshanti\nAdansi North\nAboabo Health Centre\nHealth Centre\nAboabo No 2\nGovernment\n6.22393\n-1.34982\nPOINT (-1.34982 6.22393)\n\n\n3\nAshanti\nAfigya-Kwabre\nAboabogya Health Centre\nHealth Centre\nAboabogya\nGovernment\n6.84177\n-1.61098\nPOINT (-1.61098 6.84177)\n\n\n4\nAshanti\nKwabre\nAboaso Health Centre\nHealth Centre\nAboaso\nGovernment\n6.84177\n-1.61098\nPOINT (-1.61098 6.84177)\n\n\n\n\n\n\n\n위의 코드 셀에서 CSV 파일에서 GeoDataFrame을 만들려면 Pandas와 GeoPandas를 모두 사용해야 했습니다:\n\n먼저 위도 및 경도 좌표가 포함된 열을 포함하는 데이터 프레임을 만듭니다.\n이를 지오데이터프레임으로 변환하기 위해 gpd.GeoDataFrame()을 사용합니다.\ngpd.points_from_xy() 함수는 위도 및 경도 열에서 포인트 객체를 만듭니다."
  },
  {
    "objectID": "posts/kaggle_gopandas2.html#re-projecting",
    "href": "posts/kaggle_gopandas2.html#re-projecting",
    "title": "kaggle geopandas(2) - Coordinate Reference Systems (좌표참조 시스템)",
    "section": "Re-projecting",
    "text": "Re-projecting\n재투영은 CRS를 변경하는 과정을 말합니다. 이 작업은 GeoPandas에서 to_crs() 메서드를 사용하여 수행됩니다.\n여러 GeoDataFrame을 plotting할 때는 모두 동일한 CRS를 사용하는 것이 중요합니다. 아래 코드 셀에서는 시설 GeoDataFrame의 CRS를 지역의 CRS와 일치하도록 변경한 후 플로팅합니다.\n\n# Create a map\nax = regions.plot(figsize=(8,8), color='whitesmoke', linestyle=':', edgecolor='black')\nfacilities.to_crs(epsg=32630).plot(markersize=1, ax=ax)\n\n&lt;Axes: &gt;\n\n\n\n\n\nto_crs() 메서드는 “geometry” 열만 수정하고 다른 모든 열은 그대로 유지합니다.\n\n# The \"Latitude\" and \"Longitude\" columns are unchanged\nfacilities.to_crs(epsg=32630).head()\n\n\n\n\n\n\n\n\nRegion\nDistrict\nFacilityName\nType\nTown\nOwnership\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nAshanti\nOffinso North\nA.M.E Zion Clinic\nClinic\nAfrancho\nCHAG\n7.40801\n-1.96317\nPOINT (614422.662 818986.851)\n\n\n1\nAshanti\nBekwai Municipal\nAbenkyiman Clinic\nClinic\nAnwiankwanta\nPrivate\n6.46312\n-1.58592\nPOINT (656373.863 714616.547)\n\n\n2\nAshanti\nAdansi North\nAboabo Health Centre\nHealth Centre\nAboabo No 2\nGovernment\n6.22393\n-1.34982\nPOINT (682573.395 688243.477)\n\n\n3\nAshanti\nAfigya-Kwabre\nAboabogya Health Centre\nHealth Centre\nAboabogya\nGovernment\n6.84177\n-1.61098\nPOINT (653484.490 756478.812)\n\n\n4\nAshanti\nKwabre\nAboaso Health Centre\nHealth Centre\nAboaso\nGovernment\n6.84177\n-1.61098\nPOINT (653484.490 756478.812)\n\n\n\n\n\n\n\n\n# Change the CRS to EPSG 4326\nregions.to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\").head()\n\n\n\n\n\n\n\n\nRegion\ngeometry\n\n\n\n\n0\nAshanti\nPOLYGON ((-1.30985 7.62302, -1.30786 7.62198, ...\n\n\n1\nBrong Ahafo\nPOLYGON ((-2.54567 8.76089, -2.54473 8.76071, ...\n\n\n2\nCentral\nPOLYGON ((-2.06723 6.29473, -2.06658 6.29420, ...\n\n\n3\nEastern\nPOLYGON ((-0.21751 7.21009, -0.21747 7.20993, ...\n\n\n4\nGreater Accra\nPOLYGON ((0.23456 6.10986, 0.23484 6.10974, 0...."
  },
  {
    "objectID": "posts/kaggle_gopandas2.html#attributes-of-geometric-objects-기하학적-개체-속성",
    "href": "posts/kaggle_gopandas2.html#attributes-of-geometric-objects-기하학적-개체-속성",
    "title": "kaggle geopandas(2) - Coordinate Reference Systems (좌표참조 시스템)",
    "section": "Attributes of geometric objects (기하학적 개체 속성)",
    "text": "Attributes of geometric objects (기하학적 개체 속성)\n첫 번째 튜토리얼에서 배운 것처럼 임의의 GeoDataFrame의 경우 “geometry” 열의 유형은 표시하려는 대상에 따라 달라집니다:\n\na Point for the epicenter of an earthquake,\na LineString for a street, or\na Polygon to show country boundaries.\nPoint : 지진의 진원지를 나타낸다.\nLineString : 거리\nPolygon : 국가 경계를 표시할 수 있습니다.\n\n세 가지 유형의 geometric objects(기하학적 개체) 모두 데이터 집합을 빠르게 분석하는 데 사용할 수 있는 기본 속성이 있습니다. 예를 들어, x 및 y 속성에서 각각 점의 x 및 y 좌표를 가져올 수 있습니다.\n\n# Get the x-coordinate of each point\nfacilities.geometry.head().x\n\n0   -1.96317\n1   -1.58592\n2   -1.34982\n3   -1.61098\n4   -1.61098\ndtype: float64\n\n\n길이 속성에서 LineString의 길이를 구할 수 있습니다.\n또는 면적 속성에서 Polygon 면적을 구할 수 있습니다.\n\n# Calculate the area (in square meters) of each polygon in the GeoDataFrame \nregions.loc[:, \"AREA\"] = regions.geometry.area / 10**6\n\nprint(\"Area of Ghana: {} square kilometers\".format(regions.AREA.sum()))\nprint(\"CRS:\", regions.crs)\nregions.head()\n\nArea of Ghana: 239584.5760055668 square kilometers\nCRS: epsg:32630\n\n\n\n\n\n\n\n\n\nRegion\ngeometry\nAREA\n\n\n\n\n0\nAshanti\nPOLYGON ((686446.075 842986.894, 686666.193 84...\n24379.017777\n\n\n1\nBrong Ahafo\nPOLYGON ((549970.457 968447.094, 550073.003 96...\n40098.168231\n\n\n2\nCentral\nPOLYGON ((603176.584 695877.238, 603248.424 69...\n9665.626760\n\n\n3\nEastern\nPOLYGON ((807307.254 797910.553, 807311.908 79...\n18987.625847\n\n\n4\nGreater Accra\nPOLYGON ((858081.638 676424.913, 858113.115 67...\n3706.511145\n\n\n\n\n\n\n\n위의 코드 셀에서 GeoDataFrame 지역의 CRS가 a “Mercator” projection(“메르카토르” 투영법)인 EPSG 32630으로 설정되어 있기 때문에 면적 계산이 “아프리카 알버스 등면적 원뿔” 같은 등면적 투영법을 사용한 경우보다 약간 덜 정확합니다.\n하지만 이렇게 하면 가나의 면적은 약 239585제곱킬로미터로 계산되어 정답과 크게 다르지 않습니다."
  },
  {
    "objectID": "posts/kaggle_gopandas2.html#your-turn",
    "href": "posts/kaggle_gopandas2.html#your-turn",
    "title": "kaggle geopandas(2) - Coordinate Reference Systems (좌표참조 시스템)",
    "section": "Your turn",
    "text": "Your turn\n배운 내용을 활용하여 남미로의 조류 이동을 추적하세요."
  },
  {
    "objectID": "posts/kaggle_gopandas3.html",
    "href": "posts/kaggle_gopandas3.html",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "",
    "text": "인터랙티브 히트맵, 코로페스 지도 등을 만드는 방법을 알아보세요!"
  },
  {
    "objectID": "posts/kaggle_gopandas3.html#introduction",
    "href": "posts/kaggle_gopandas3.html#introduction",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "Introduction",
    "text": "Introduction\n이 튜토리얼에서는 folium 패키지로 interactive maps을 만드는 방법을 배웁니다. 그 과정에서 새로운 기술을 적용하여 보스턴 범죄 데이터를 시각화하게 됩니다.\n\nimport pandas as pd\nimport geopandas as gpd\nimport math\n\nC:\\Users\\heeyoung\\.conda\\envs\\quarto\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.1-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n  warnings.warn(\n\n\n\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster"
  },
  {
    "objectID": "posts/kaggle_gopandas3.html#your-first-interactive-map",
    "href": "posts/kaggle_gopandas3.html#your-first-interactive-map",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "Your first interactive map",
    "text": "Your first interactive map\n먼저 folium.Map()을 사용하여 비교적 간단한 지도를 만듭니다.\n\n# Create a map\nm_1 = folium.Map(location=[42.32,-71.0589], tiles='openstreetmap', zoom_start=10)\n\n# Display the map\nm_1\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n여러 인수를 사용하여 지도의 모양을 사용자 지정할 수 있습니다:\n\n위치는 맵의 초기 중심을 설정합니다. 여기서는 보스턴시의 위도(42.32° N)와 경도(-71.0589° E)를 사용합니다.\n타일은 지도의 스타일을 변경하며, 이 경우 OpenStreetMap 스타일을 선택합니다. 다른 옵션이 궁금하다면 여기에 나열된 다른 옵션을 찾아볼 수 있습니다.\nzoom_start는 지도의 초기 확대/축소 수준을 설정하며, 값이 클수록 지도가 더 가까이 확대됩니다.\n\n지금 바로 확대/축소하거나 지도를 다른 방향으로 드래그하여 탐색해 보세요.\n\n### create a map object with OpenStreeMap tiles\nm = folium.Map()"
  },
  {
    "objectID": "posts/kaggle_gopandas3.html#the-data",
    "href": "posts/kaggle_gopandas3.html#the-data",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "The data",
    "text": "The data\n이제 맵에 범죄 데이터를 추가하겠습니다!\n여기서는 데이터 로드 단계에 집중하지 않겠습니다. 대신, 이미 Pandas Dataframe에 범죄 데이터가 있는 지점에 있다고 상상해 보겠습니다. 데이터의 처음 다섯 행은 아래와 같습니다.\n\n# Load the data\ncrimes = pd.read_csv(\"geopandas_data/crimes-in-boston/crimes-in-boston/crime.csv\", encoding='latin-1')\n\n# Drop rows with missing locations\ncrimes.dropna(subset=['Lat', 'Long', 'DISTRICT'], inplace=True)\n\n# Focus on major crimes in 2018\ncrimes = crimes[crimes.OFFENSE_CODE_GROUP.isin([\n    'Larceny', 'Auto Theft', 'Robbery', 'Larceny From Motor Vehicle', 'Residential Burglary',\n    'Simple Assault', 'Harassment', 'Ballistics', 'Aggravated Assault', 'Other Burglary', \n    'Arson', 'Commercial Burglary', 'HOME INVASION', 'Homicide', 'Criminal Harassment', \n    'Manslaughter'])]\ncrimes = crimes[crimes.YEAR&gt;=2018]\n\n# Print the first five rows of the table\ncrimes.head()\n\n\n\n\n\n\n\n\nINCIDENT_NUMBER\nOFFENSE_CODE\nOFFENSE_CODE_GROUP\nOFFENSE_DESCRIPTION\nDISTRICT\nREPORTING_AREA\nSHOOTING\nOCCURRED_ON_DATE\nYEAR\nMONTH\nDAY_OF_WEEK\nHOUR\nUCR_PART\nSTREET\nLat\nLong\nLocation\n\n\n\n\n0\nI182070945\n619\nLarceny\nLARCENY ALL OTHERS\nD14\n808\nNaN\n2018-09-02 13:00:00\n2018\n9\nSunday\n13\nPart One\nLINCOLN ST\n42.357791\n-71.139371\n(42.35779134, -71.13937053)\n\n\n6\nI182070933\n724\nAuto Theft\nAUTO THEFT\nB2\n330\nNaN\n2018-09-03 21:25:00\n2018\n9\nMonday\n21\nPart One\nNORMANDY ST\n42.306072\n-71.082733\n(42.30607218, -71.08273260)\n\n\n8\nI182070931\n301\nRobbery\nROBBERY - STREET\nC6\n177\nNaN\n2018-09-03 20:48:00\n2018\n9\nMonday\n20\nPart One\nMASSACHUSETTS AVE\n42.331521\n-71.070853\n(42.33152148, -71.07085307)\n\n\n19\nI182070915\n614\nLarceny From Motor Vehicle\nLARCENY THEFT FROM MV - NON-ACCESSORY\nB2\n181\nNaN\n2018-09-02 18:00:00\n2018\n9\nSunday\n18\nPart One\nSHIRLEY ST\n42.325695\n-71.068168\n(42.32569490, -71.06816778)\n\n\n24\nI182070908\n522\nResidential Burglary\nBURGLARY - RESIDENTIAL - NO FORCE\nB2\n911\nNaN\n2018-09-03 18:38:00\n2018\n9\nMonday\n18\nPart One\nANNUNCIATION RD\n42.335062\n-71.093168\n(42.33506218, -71.09316781)"
  },
  {
    "objectID": "posts/kaggle_gopandas3.html#plotting-points",
    "href": "posts/kaggle_gopandas3.html#plotting-points",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "Plotting points",
    "text": "Plotting points\n지도에 넣어야 하는 데이터의 양을 줄이기 위해 (일시적으로) 주간 강도 사건에 집중할 것입니다.\n\ndaytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & \\\n                            (crimes.HOUR.isin(range(9,18))))]\n\n\nfolium.Marker\nfolium.Marker()로 지도에 마커를 추가합니다. 아래의 각 마커는 서로 다른 강도에 해당합니다.\n\n# Create a map\nm_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)\n\n# Add points to the map\nfor idx, row in daytime_robberies.iterrows():\n    Marker([row['Lat'], row['Long']]).add_to(m_2)\n\n# Display the map\nm_2\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nfolium.plugins.MarkerCluster\n추가할 마커가 많은 경우 folium.plugins.MarkerCluster()를 사용하면 지도를 깔끔하게 정리하는 데 도움이 될 수 있습니다. 각 마커는 MarkerCluster 객체에 추가됩니다.\n\n# Create the map\nm_3 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)\n\n# Add points to the map\nmc = MarkerCluster()\nfor idx, row in daytime_robberies.iterrows():\n    if not math.isnan(row['Long']) and not math.isnan(row['Lat']):\n        mc.add_child(Marker([row['Lat'], row['Long']]))\nm_3.add_child(mc)\n\n# Display the map\nm_3\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nBubble maps\n버블 맵은 마커 대신 원을 사용합니다. 각 원의 크기와 색상을 변경하여 위치와 다른 두 변수 사이의 관계를 표시할 수도 있습니다.\nfolium.Circle()을 사용하여 원을 반복적으로 추가하여 버블 맵을 만듭니다. 아래 코드 셀에서 9~12시간에 발생한 강도는 녹색으로 표시되고, 13~17시간에 발생한 강도는 빨간색으로 표시됩니다.\n\n# Create a base map\nm_4 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)\n\ndef color_producer(val):\n    if val &lt;= 12:\n        return 'forestgreen'\n    else:\n        return 'darkred'\n\n# Add a bubble map to the base map\nfor i in range(0,len(daytime_robberies)):\n    Circle(\n        location=[daytime_robberies.iloc[i]['Lat'], daytime_robberies.iloc[i]['Long']],\n        radius=20,\n        color=color_producer(daytime_robberies.iloc[i]['HOUR'])).add_to(m_4)\n\n# Display the map\nm_4\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nfolium.Circle()은 여러 인수를 받습니다:\n\n위치는 원의 중심을 위도와 경도로 표시한 목록입니다.\nradius는 원의 반지름을 설정합니다.\n\n기존 버블 맵에서는 각 원의 반지름이 달라질 수 있습니다. 각 원의 색상을 변경하는 데 사용되는 color_producer() 함수와 유사한 함수를 정의하여 이를 구현할 수 있습니다.\n\ncolor는 각 원의 색상을 설정합니다.\n\ncolor_producer() 함수는 강도의 위치에 대한 시간의 효과를 시각화하는 데 사용됩니다."
  },
  {
    "objectID": "posts/kaggle_gopandas3.html#heatmaps",
    "href": "posts/kaggle_gopandas3.html#heatmaps",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "Heatmaps",
    "text": "Heatmaps\n히트맵을 생성하기 위해 folium.plugins.HeatMap()을 사용합니다. 이것은 도시 내 여러 지역의 범죄 밀도를 보여주며, 빨간색 영역은 상대적으로 범죄 발생이 더 많습니다.\n대도시에서 예상할 수 있듯이 대부분의 범죄는 도심 근처에서 발생합니다.\n\n# Create a base map\nm_5 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)\n\n# Add a heatmap to the base map\nHeatMap(data=crimes[['Lat', 'Long']], radius=10).add_to(m_5)\n\n# Display the map\nm_5\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n위의 코드 셀에서 볼 수 있듯이 folium.plugins.HeatMap()은 몇 가지 인수를 받습니다:\n\ndata는 플롯하고자 하는 위치가 포함된 데이터프레임입니다.\n반경은 히트맵의 부드러움을 제어합니다. 값이 클수록 히트맵이 더 부드러워집니다(즉, 간격이 줄어듭니다)."
  },
  {
    "objectID": "posts/kaggle_gopandas3.html#choropleth-maps",
    "href": "posts/kaggle_gopandas3.html#choropleth-maps",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "Choropleth maps",
    "text": "Choropleth maps\n경찰 구역별로 범죄가 어떻게 다른지 이해하기 위해 코로플라스 맵을 만들어 보겠습니다.\n첫 번째 단계로, 각 구역에 서로 다른 행이 할당되고 ‘geometry’ 열에 지리적 경계가 포함된 지오데이터프레임(GeoDataFrame)을 만듭니다.\n\n# GeoDataFrame with geographical boundaries of Boston police districts\ndistricts_full = gpd.read_file('geopandas_data/Police_Districts/Police_Districts/Police_Districts.shp')\ndistricts = districts_full[[\"DISTRICT\", \"geometry\"]].set_index(\"DISTRICT\")\ndistricts.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\nDISTRICT\n\n\n\n\n\nA15\nMULTIPOLYGON (((-71.07416 42.39051, -71.07415 ...\n\n\nA7\nMULTIPOLYGON (((-70.99644 42.39557, -70.99644 ...\n\n\nA1\nPOLYGON ((-71.05200 42.36884, -71.05169 42.368...\n\n\nC6\nPOLYGON ((-71.04406 42.35403, -71.04412 42.353...\n\n\nD4\nPOLYGON ((-71.07416 42.35724, -71.07359 42.357...\n\n\n\n\n\n\n\n또한 각 지역의 범죄 발생 건수를 보여주는 plot_dict라는 Pandas Series를 만듭니다.\n\n# Number of crimes in each police district\nplot_dict = crimes.DISTRICT.value_counts()\nplot_dict.head()\n\nDISTRICT\nD4     2885\nB2     2231\nA1     2130\nC11    1899\nB3     1421\nName: count, dtype: int64\n\n\n이것은 코드가 지리적 경계를 적절한 색상과 일치시키는 방법을 알기 때문에 plot_dict의 인덱스가 구역과 동일한 것이 매우 중요합니다.\nfolium.Choropleth() 클래스를 사용하여 코로플라스 지도를 만들 수 있습니다. 아래 지도가 렌더링되지 않는 경우 다른 웹 브라우저에서 페이지를 확인해 보세요.\n\n# Create a base map\nm_6 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)\n\n# Add a choropleth map to the base map\nChoropleth(geo_data=districts.__geo_interface__, \n           data=plot_dict, \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='Major criminal incidents (Jan-Aug 2018)'\n          ).add_to(m_6)\n\n# Display the map\nm_6\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n가장 안전한지역을 초록, 위험한 지역을 빨간색으로 바꿔봅시다.\nfolium.Choropleth()는 여러 인수를 받습니다:\n\ngeo_data는 각 지리적 영역의 경계를 포함하는 GeoJSON FeatureCollection입니다.\n\n위의 코드에서는 geo_interface 속성을 사용하여 지역 GeoDataFrame을 GeoJSON FeatureCollection으로 변환합니다.\n\n데이터는 각 지리적 영역에 색을 입히는 데 사용할 값이 포함된 Pandas 시리즈입니다.\nkey_on은 항상 feature.id로 설정됩니다.\n\n이는 geo_data에 사용되는 GeoDataFrame과 데이터에 제공된 판다 시리즈의 인덱스가 동일하다는 사실을 나타냅니다. 자세한 내용을 이해하려면 GeoJSON Feature Collection의 구조를 좀 더 자세히 살펴봐야 합니다(“features” 키에 해당하는 값은 목록이고 각 항목은 “id” 키를 포함하는 사전입니다).\n\nfill_color는 색상 배율을 설정합니다.\nlegend_name은 맵의 오른쪽 상단 모서리에 있는 범례에 레이블을 지정합니다."
  },
  {
    "objectID": "posts/kaggle_gopandas3.html#your-turn",
    "href": "posts/kaggle_gopandas3.html#your-turn",
    "title": "kaggle geopandas(3) - Interactive Maps",
    "section": "Your turn",
    "text": "Your turn\n나만의 지도를 디자인하여 일본의 어느 지역에 추가 지진 보강이 필요한지 알아보세요."
  },
  {
    "objectID": "posts/numpy.html",
    "href": "posts/numpy.html",
    "title": "Numpy Code!",
    "section": "",
    "text": "“numpy 기본 코드 실습(한글)”\n\n\ntoc:true\nbranch: master\nbadges: true\ncomments: true\ncategories: [jupyter, python]\n\n도구 - 넘파이(NumPy)\n*넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.”\n\n\n\n&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/handson-ml2/blob/master/tools_numpy.ipynb\"&gt;&lt;img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /&gt;구글 코랩에서 실행하기&lt;/a&gt;"
  },
  {
    "objectID": "posts/numpy.html#np.zeros",
    "href": "posts/numpy.html#np.zeros",
    "title": "Numpy Code!",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\nnp.zeros(5)\narray([0., 0., 0., 0., 0.])\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.zeros((3,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/numpy.html#용어",
    "href": "posts/numpy.html#용어",
    "title": "Numpy Code!",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\na = np.zeros((3,4))\na\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\na.shape\n(3, 4)\n\na.ndim  # len(a.shape)와 같습니다\n2\n\na.size\n12"
  },
  {
    "objectID": "posts/numpy.html#n-차원-배열",
    "href": "posts/numpy.html#n-차원-배열",
    "title": "Numpy Code!",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\nnp.zeros((2,3,4))\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "posts/numpy.html#배열-타입",
    "href": "posts/numpy.html#배열-타입",
    "title": "Numpy Code!",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\ntype(np.zeros((3,4)))\nnumpy.ndarray"
  },
  {
    "objectID": "posts/numpy.html#np.ones",
    "href": "posts/numpy.html#np.ones",
    "title": "Numpy Code!",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.ones((3,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/numpy.html#np.full",
    "href": "posts/numpy.html#np.full",
    "title": "Numpy Code!",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\nnp.full((3,4), np.pi)\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "posts/numpy.html#np.empty",
    "href": "posts/numpy.html#np.empty",
    "title": "Numpy Code!",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\nnp.empty((2,3))\narray([[0.        , 0.33333333, 0.66666667],\n       [1.        , 1.33333333, 1.66666667]])"
  },
  {
    "objectID": "posts/numpy.html#np.array",
    "href": "posts/numpy.html#np.array",
    "title": "Numpy Code!",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "posts/numpy.html#np.arange",
    "href": "posts/numpy.html#np.arange",
    "title": "Numpy Code!",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\nnp.arange(1, 5)\narray([1, 2, 3, 4])\n\n부동 소수도 가능합니다:\nnp.arange(1.0, 5.0)\narray([1., 2., 3., 4.])\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\nnp.arange(1, 5, 0.5)\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]\n\nfor loops를 사용하지 않고 전체 array에 대한 연산 수행이 가능합니다.\n평균적으로 Numpy-based 알고리즘은 10~100배정도 속도가 더 빠르고 적은 메모리를 사용합니다.\nmy_arr = np.arange(1000000)\nmy_list = list(range(1000000))\n\n%time for _ in range(10): my_arr2 = my_arr * 2\n%time for _ in range(10): my_list2 = [x * 2 for x in my_list]\nCPU times: user 15.1 ms, sys: 4.18 ms, total: 19.3 ms\nWall time: 19.5 ms\nCPU times: user 741 ms, sys: 165 ms, total: 907 ms\nWall time: 913 ms\n\nFor loop를 돌릴 때의 속도 비교\nsize = 10\nfor x in range(size): x ** 2\nimport sys\n\nsize = 10\n\n%timeit for x in range(size): x ** 2\n# out: 10 loops, best of 3: 136 ms per loop\n\n# avoid this\n%timeit for x in np.arange(size): x ** 2\n#out: 1 loops, best of 3: 1.16 s per loop\n\n# use this\n%timeit np.arange(size) ** 2\n#out: 100 loops, best of 3: 19.5 ms per loop\n1.77 µs ± 3.95 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n2.73 µs ± 66.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n942 ns ± 7.45 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)"
  },
  {
    "objectID": "posts/numpy.html#np.linspace",
    "href": "posts/numpy.html#np.linspace",
    "title": "Numpy Code!",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\nprint(np.linspace(0, 5/3, 6))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "posts/numpy.html#np.rand와-np.randn",
    "href": "posts/numpy.html#np.rand와-np.randn",
    "title": "Numpy Code!",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다.\n예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\nnp.random.rand(3,4)\narray([[0.37892456, 0.17966937, 0.38206837, 0.34922123],\n       [0.80462136, 0.9845914 , 0.9416127 , 0.28305275],\n       [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]])\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\nnp.random.randn(3,4)\narray([[ 0.83811287, -0.57131751, -0.4381827 ,  1.1485899 ],\n       [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ],\n       [ 1.01003549,  1.04381736, -0.93060038,  2.39043293]])\n\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "posts/numpy.html#np.fromfunction",
    "href": "posts/numpy.html#np.fromfunction",
    "title": "Numpy Code!",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "posts/numpy.html#dtype",
    "href": "posts/numpy.html#dtype",
    "title": "Numpy Code!",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\nc = np.arange(1, 5)\nprint(c.dtype, c)\nint32 [1 2 3 4]\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\nfloat64 [1. 2. 3. 4.]\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/numpy.html#itemsize",
    "href": "posts/numpy.html#itemsize",
    "title": "Numpy Code!",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n8"
  },
  {
    "objectID": "posts/numpy.html#data-버퍼",
    "href": "posts/numpy.html#data-버퍼",
    "title": "Numpy Code!",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "posts/numpy.html#자신을-변경",
    "href": "posts/numpy.html#자신을-변경",
    "title": "Numpy Code!",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\n\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\n\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3\n\ng[1,1,1]\n17"
  },
  {
    "objectID": "posts/numpy.html#reshape",
    "href": "posts/numpy.html#reshape",
    "title": "Numpy Code!",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n\ng[0,0,0] = 10\ng2\narray([[10,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\ng2[1, 2] = 999\ng2\narray([[ 10,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n이에 상응하는 g의 원소도 수정됩니다.\ng\narray([[[ 10,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])\n\n완전히 다른 공간에 값만 같게 복사를 하고 싶다면 copy를 사용.\n이렇게 할 경우 두 객체는 독립적인 객체로 존재함\ng2 = g.copy()"
  },
  {
    "objectID": "posts/numpy.html#ravel",
    "href": "posts/numpy.html#ravel",
    "title": "Numpy Code!",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\ng.ravel()\narray([ 10,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "posts/numpy.html#규칙-1",
    "href": "posts/numpy.html#규칙-1",
    "title": "Numpy Code!",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\nh = np.arange(5).reshape(1, 1, 5)\nh\narray([[[0, 1, 2, 3, 4]]])\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "posts/numpy.html#규칙-2",
    "href": "posts/numpy.html#규칙-2",
    "title": "Numpy Code!",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\nk = np.arange(6).reshape(2, 3)\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n[[100], [200]]\n[[100], [200]]\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n(2,3) 크기의 ndarray에 (3,) 크기의 ndarray 더하기\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\narray([[100, 201, 302],\n       [103, 204, 305]])\n\ntest = np.array([100, 200, 300])\ntest.shape\ntest\narray([100, 200, 300])\n\n# step 1\ntest = test.reshape(1,3)\ntest\narray([[100, 200, 300]])\n\n# step 2\nnp.vstack((test,test))\narray([[100, 200, 300],\n       [100, 200, 300]])\n\n# step 2\nnp.concatenate((test,test),axis=0)\narray([[100, 200, 300],\n       [100, 200, 300]])\n\n또 매우 간단히 다음 처럼 해도 됩니다:\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "posts/numpy.html#규칙-3",
    "href": "posts/numpy.html#규칙-3",
    "title": "Numpy Code!",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\noperands could not be broadcast together with shapes (2,3) (2,) \n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요.\na = np.array([[0.0],[10.0],[20.0],[30.0]])\na\narray([[ 0.],\n       [10.],\n       [20.],\n       [30.]])\n\na = np.array([0.0, 10.0, 20.0, 30.0])\nb = np.array([1.0, 2.0, 3.0])\na[:, np.newaxis] + b\narray([[ 1.,  2.,  3.],\n       [11., 12., 13.],\n       [21., 22., 23.],\n       [31., 32., 33.]])"
  },
  {
    "objectID": "posts/numpy.html#업캐스팅",
    "href": "posts/numpy.html#업캐스팅",
    "title": "Numpy Code!",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\nuint8 [0 1 2 3 4]\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\nint16 [ 5  7  9 11 13]\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "posts/numpy.html#ndarray-메서드",
    "href": "posts/numpy.html#ndarray-메서드",
    "title": "Numpy Code!",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\nc=np.arange(24).reshape(2,3,4)\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\nc.sum(axis=2) \narray([[ 6, 22, 38],\n       [54, 70, 86]])\n\n여러 축에 대해서 더할 수도 있습니다:\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\narray([ 60,  92, 124])\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n(60, 92, 124)"
  },
  {
    "objectID": "posts/numpy.html#일반-함수",
    "href": "posts/numpy.html#일반-함수",
    "title": "Numpy Code!",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n다음은 유용한 단항 일반 함수들입니다:\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n:5: RuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\n:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "posts/numpy.html#이항-일반-함수",
    "href": "posts/numpy.html#이항-일반-함수",
    "title": "Numpy Code!",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\narray([ 3,  6,  2, 11])\n\nnp.greater(a, b)  # a &gt; b 와 동일\narray([False, False,  True, False])\n\nnp.maximum(a, b)\narray([2, 8, 3, 7])\n\nnp.copysign(a, b)\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "posts/numpy.html#차원-배열",
    "href": "posts/numpy.html#차원-배열",
    "title": "Numpy Code!",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n19\n\na[2:5]\narray([ 3, 19, 13])\n\na[2:-1]\narray([ 3, 19, 13,  7])\n\na[:2]\narray([1, 5])\n\na[2::2]\narray([ 3, 13,  3])\n\na[::-1]\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n물론 원소를 수정할 수 있죠:\na[3]=999\na\narray([  1,   5,   3, 999,  13,   7,   3])\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\na[2:5] = [997, 998, 999]\na\narray([  1,   5, 997, 998, 999,   7,   3])\n\nQuiz. 아래의 array를 사용해서 다음 퀴즈를 풀어봅시다.\nimport numpy as np\n\narray_2d = np.array([[5, 10, 15],\n                     [20, 25, 30],\n                     [35, 40, 45]])\n\n2차원 배열 ’array_2d’에서 첫 번째 행(row)의 모든 요소를 선택해 보세요.\n\n힌트: 인덱싱을 사용하여 첫 번째 행을 선택할 수 있습니다.\n\n\narray_2d[0,:]\narray([ 5, 10, 15])\n\n\n2차원 배열 ’array_2d’에서 두 번째 열(column)의 모든 요소를 선택해 보세요.\n\n힌트: 인덱싱과 슬라이싱을 사용하여 두 번째 열을 선택할 수 있습니다.\n\n\narray_2d[:,1]\narray([10, 25, 40])\n\n\n2차원 배열 ’array_2d’에서 다음 요소들을 선택해 보세요: 25, 30, 40, 45\n\n힌트: 팬시 인덱싱(fancy indexing)을 사용하여 여러 요소를 한 번에 선택할 수 있습니다.\n\n\narray_2d[(1,2),1:3]\narray([[25, 30],\n       [40, 45]])"
  },
  {
    "objectID": "posts/numpy.html#보통의-파이썬-배열과-차이점",
    "href": "posts/numpy.html#보통의-파이썬-배열과-차이점",
    "title": "Numpy Code!",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\na\narray([ 1,  5,  3, 19, 13,  7,  3])\n\na[2:5] = -1\na\narray([ 1,  5, -1, -1, -1,  7,  3])\n\nList는 브로드캐스팅으로 할당이 안됨\nb = [1, 5, 3, 19, 13, 7, 3]\nb[2:5] = -1\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\ncould not broadcast input array from shape (6,) into shape (3,)\n\n원소를 삭제할 수도 없습니다:\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\ncannot delete array elements\n\nList에서는 삭제가 가능\nb = [1, 5, 3, 19, 13, 7, 3] \ndel b[2:5]\nb\n[1, 5, 7, 3]\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\narray([  -1, 2000,   -1,    7])\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "posts/numpy.html#다차원-배열",
    "href": "posts/numpy.html#다차원-배열",
    "title": "Numpy Code!",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\nb[1, 2]  # 행 1, 열 2\n14\n\nb[1, :]  # 행 1, 모든 열\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\nb[:, 1]  # 모든 행, 열 1\narray([ 1, 13, 25, 37])\n\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\nb[1, :].shape\n(12,)\n\nb[1:2, :]\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "posts/numpy.html#팬시-인덱싱fancy-indexing",
    "href": "posts/numpy.html#팬시-인덱싱fancy-indexing",
    "title": "Numpy Code!",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "posts/numpy.html#고차원",
    "href": "posts/numpy.html#고차원",
    "title": "Numpy Code!",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\nc = b.reshape(4,2,6)\nc\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n34\n\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\narray([27, 33])\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "posts/numpy.html#생략-부호-...",
    "href": "posts/numpy.html#생략-부호-...",
    "title": "Numpy Code!",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\narray([30, 31, 32, 33, 34, 35])\n\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\narray([27, 33])\n\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "posts/numpy.html#불리언-인덱싱",
    "href": "posts/numpy.html#불리언-인덱싱",
    "title": "Numpy Code!",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "posts/numpy.html#np.ix_",
    "href": "posts/numpy.html#np.ix_",
    "title": "Numpy Code!",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\nb[np.ix_((0,2),(1,4,7,10))]\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\nb[np.ix_(rows_on, cols_on)]\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\nnp.ix_(rows_on, cols_on)\n(array([[0],\n        [2]], dtype=int64),\n array([[ 1,  4,  7, 10]], dtype=int64))\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\nb.shape\n(4, 12)\n\nb[b % 3 == 1]\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "posts/numpy.html#vstack",
    "href": "posts/numpy.html#vstack",
    "title": "Numpy Code!",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\nq4 = np.vstack((q1, q2, q3))\nq4\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\nq4.shape\n(10, 4)\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "posts/numpy.html#hstack",
    "href": "posts/numpy.html#hstack",
    "title": "Numpy Code!",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\nq5 = np.hstack((q1, q3))\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\nq5.shape\n(3, 8)\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "posts/numpy.html#concatenate",
    "href": "posts/numpy.html#concatenate",
    "title": "Numpy Code!",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\nq7.shape\n(10, 4)\n\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다.\nq5 = np.hstack((q1, q3))\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\nnp.concatenate((q1, q3), axis=1)\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])"
  },
  {
    "objectID": "posts/numpy.html#stack",
    "href": "posts/numpy.html#stack",
    "title": "Numpy Code!",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\nq1.shape\n(3, 4)\n\nq3.shape\n(3, 4)\n\nq8 = np.stack((q1, q3))\nq8\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\n\nq8.shape\n(2, 3, 4)"
  },
  {
    "objectID": "posts/numpy.html#행렬-전치",
    "href": "posts/numpy.html#행렬-전치",
    "title": "Numpy Code!",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\nm1 = np.arange(10).reshape(2,5)\nm1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\nm1.T\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\nm2 = np.arange(5)\nm2\narray([0, 1, 2, 3, 4])\n\nm2.T\narray([0, 1, 2, 3, 4])\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\nm2r = m2.reshape(1,5)\nm2r\narray([[0, 1, 2, 3, 4]])\n\nm2r.T\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "posts/numpy.html#행렬-곱셈",
    "href": "posts/numpy.html#행렬-곱셈",
    "title": "Numpy Code!",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\nn1 = np.arange(10).reshape(2, 5)\nn1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\nn2 = np.arange(15).reshape(5,3)\nn2\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\nn1.dot(n2)\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "posts/numpy.html#역행렬과-유사-역행렬",
    "href": "posts/numpy.html#역행렬과-유사-역행렬",
    "title": "Numpy Code!",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\nlinalg.inv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\nlinalg.pinv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "posts/numpy.html#단위-행렬",
    "href": "posts/numpy.html#단위-행렬",
    "title": "Numpy Code!",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\nm3.dot(linalg.inv(m3))\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\nnp.eye(3)\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "posts/numpy.html#qr-분해",
    "href": "posts/numpy.html#qr-분해",
    "title": "Numpy Code!",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\nq, r = linalg.qr(m3)\nq\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\nr\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\nq.dot(r)  # q.r는 m3와 같습니다\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "posts/numpy.html#행렬식",
    "href": "posts/numpy.html#행렬식",
    "title": "Numpy Code!",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\nlinalg.det(m3)  # 행렬식 계산\n43.99999999999997"
  },
  {
    "objectID": "posts/numpy.html#고윳값과-고유벡터",
    "href": "posts/numpy.html#고윳값과-고유벡터",
    "title": "Numpy Code!",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\narray([42.26600592, -0.35798416, -2.90802176])\n\neigenvectors # v\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "posts/numpy.html#특잇값-분해",
    "href": "posts/numpy.html#특잇값-분해",
    "title": "Numpy Code!",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\nU, S_diag, V = linalg.svd(m4)\nU\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\nS_diag\narray([3.        , 2.23606798, 2.        , 0.        ])\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\nV\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\nU.dot(S).dot(V) # U.Σ.V == m4\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/numpy.html#대각원소와-대각합",
    "href": "posts/numpy.html#대각원소와-대각합",
    "title": "Numpy Code!",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\narray([ 1,  7, 31])\n\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n39"
  },
  {
    "objectID": "posts/numpy.html#선형-방정식-풀기",
    "href": "posts/numpy.html#선형-방정식-풀기",
    "title": "Numpy Code!",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\narray([-3.,  2.])\n\nsolution을 확인해 보죠:\ncoeffs.dot(solution), depvars  # 네 같네요\n(array([ 6., -9.]), array([ 6, -9]))\n\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\nnp.allclose(coeffs.dot(solution), depvars)\nTrue"
  },
  {
    "objectID": "posts/numpy.html#바이너리-.npy-포맷",
    "href": "posts/numpy.html#바이너리-.npy-포맷",
    "title": "Numpy Code!",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\na = np.random.rand(2,3)\na\narray([[0.3825616 , 0.09137047, 0.39513439],\n       [0.26849157, 0.57397999, 0.34640211]])\n\nnp.save(\"my_array\", a)\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\nb”93NUMPY00v ?8d9Q??1q-3??8f06r?”\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\na_loaded = np.load(\"my_array.npy\")\na_loaded\narray([[0.3825616 , 0.09137047, 0.39513439],\n       [0.26849157, 0.57397999, 0.34640211]])"
  },
  {
    "objectID": "posts/numpy.html#텍스트-포맷",
    "href": "posts/numpy.html#텍스트-포맷",
    "title": "Numpy Code!",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\nnp.savetxt(\"my_array.csv\", a)\n파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n2.532406372578097642e-01 6.865321730222021523e-01 8.263373302242510432e-01\n7.562472557297507114e-01 8.915750707038208045e-01 5.886237361025211667e-01\n\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\narray([[0.25324064, 0.68653217, 0.82633733],\n       [0.75624726, 0.89157507, 0.58862374]])"
  },
  {
    "objectID": "posts/numpy.html#압축된-.npz-포맷",
    "href": "posts/numpy.html#압축된-.npz-포맷",
    "title": "Numpy Code!",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x007\\\\x16\\\\xcb\\\\xb7\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n\n다음과 같이 이 파일을 로드할 수 있습니다:\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\nlist(my_arrays.keys())\n['my_a', 'my_b']\n\nmy_arrays[\"my_a\"]\narray([[0.15661405, 0.58827297, 0.04122154],\n       [0.55722113, 0.62681091, 0.58300341]])"
  },
  {
    "objectID": "posts/Pandas.html",
    "href": "posts/Pandas.html",
    "title": "pandas Code! (1)",
    "section": "",
    "text": "“pandas 기본 코드 실습(한글)”\n\n\ntoc:true\nbranch: master\nbadges: true\ncomments: true\nauthor: Jiho Yeo\ncategories: [jupyter, python]\n\n도구 - 판다스(pandas)\npandas 라이브러리는 사용하기 쉬운 고성능 데이터 구조와 데이터 분석 도구를 제공합니다. 주 데이터 구조는 DataFrame입니다. 이를 인-메모리(in-memory) 2D 테이블로 생각할 수 있습니다(열 이름과 행 레이블이 있는 스프레드시트와 비슷합니다). 엑셀에 있는 많은 기능을 프로그램에서 사용할 수 있습니다. 여기에는 피봇 테이블이나 다른 열을 기반으로 열을 계산하고 그래프 출력하는 기능 등이 포함됩니다. 열 값으로 행을 그룹핑할 수도 있습니다. 또한 SQL과 비슷하게 테이블을 조인할 수 있습니다. 판다스는 시계열 데이터를 다루는데도 뛰어납니다.\n필요 라이브러리:\n\n넘파이(NumPy) – 넘파이에 익숙하지 않다면 지금 넘파이 튜토리얼을 둘러 보세요.\n\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "posts/Pandas.html#series-만들기",
    "href": "posts/Pandas.html#series-만들기",
    "title": "pandas Code! (1)",
    "section": "Series 만들기",
    "text": "Series 만들기\n첫 번째 Series 객체를 만들어 보죠!\n\nimport numpy as np\nnp.array([2,-1,3,5])\n\narray([ 2, -1,  3,  5])\n\n\n\ns = pd.Series([2,-1,3,5])\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "posts/Pandas.html#d-ndarray와-비슷합니다",
    "href": "posts/Pandas.html#d-ndarray와-비슷합니다",
    "title": "pandas Code! (1)",
    "section": "1D ndarray와 비슷합니다",
    "text": "1D ndarray와 비슷합니다\nSeries 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다:\n\nimport numpy as np\nnp.exp(s)\n\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\n\n\nSeries 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다:\n\ns + [1000,2000,3000,4000]\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다:\n\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\n\ns + 1000\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n*나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다:\n\ns &lt; 0\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "posts/Pandas.html#인덱스-레이블",
    "href": "posts/Pandas.html#인덱스-레이블",
    "title": "pandas Code! (1)",
    "section": "인덱스 레이블",
    "text": "인덱스 레이블\nSeries 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다:\n\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n그다음 dict처럼 Series를 사용할 수 있습니다:\n\ns2[\"bob\"]\n\n83\n\n\n일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다:\n\ns2[1]\n\n83\n\n\n레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다:\n\ns2.loc[\"bob\"]\n\n83\n\n\n\ns2.iloc[1]\n\n83\n\n\nSeries는 인덱스 레이블을 슬라이싱할 수도 있습니다:\n\ns2.iloc[1:3]\n\nbob         83\ncharles    112\ndtype: int64\n\n\n기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다:\n\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\n\n\n\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n2    1002\n3    1003\ndtype: int64\n\n\n보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다:\n\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 0\n\n\n하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다:\n\nsurprise_slice.iloc[0]\n\n1002"
  },
  {
    "objectID": "posts/Pandas.html#dict에서-초기화",
    "href": "posts/Pandas.html#dict에서-초기화",
    "title": "pandas Code! (1)",
    "section": "dict에서 초기화",
    "text": "dict에서 초기화\ndict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다:\n\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nSeries에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다:\n\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "posts/Pandas.html#자동-정렬",
    "href": "posts/Pandas.html#자동-정렬",
    "title": "pandas Code! (1)",
    "section": "자동 정렬",
    "text": "자동 정렬\n여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다.\n\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\n\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\n만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 \"colin\"이 없고 s3에 \"charles\"가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다).\n자동 정렬은 구조가 다르고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다:\n\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\nprint(\"s5 =\", s5.values)\n\ns2 + s5\n\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\n레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다."
  },
  {
    "objectID": "posts/Pandas.html#스칼라로-초기화",
    "href": "posts/Pandas.html#스칼라로-초기화",
    "title": "pandas Code! (1)",
    "section": "스칼라로 초기화",
    "text": "스칼라로 초기화\n스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다.\n\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "posts/Pandas.html#series-이름",
    "href": "posts/Pandas.html#series-이름",
    "title": "pandas Code! (1)",
    "section": "Series 이름",
    "text": "Series 이름\nSeries는 name을 가질 수 있습니다:\n\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\n\nbob      83\nalice    68\nName: weights, dtype: int64\n\n\n\ns6.name\n\n'weights'"
  },
  {
    "objectID": "posts/Pandas.html#series-그래프-출력",
    "href": "posts/Pandas.html#series-그래프-출력",
    "title": "pandas Code! (1)",
    "section": "Series 그래프 출력",
    "text": "Series 그래프 출력\n맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다:\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\ns7.plot()\nplt.show()\n\n\n\n\n데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요."
  },
  {
    "objectID": "posts/Pandas.html#시간-범위",
    "href": "posts/Pandas.html#시간-범위",
    "title": "pandas Code! (1)",
    "section": "시간 범위",
    "text": "시간 범위\n먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다.\n\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\n\npd.date_range('2020-10-07', '2020-10-20', freq='3D')\n\nDatetimeIndex(['2020-10-07', '2020-10-10', '2020-10-13', '2020-10-16',\n               '2020-10-19'],\n              dtype='datetime64[ns]', freq='3D')\n\n\n이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다:\n\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n이 시리즈를 그래프로 출력해 보죠:\n\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/Pandas.html#리샘플링",
    "href": "posts/Pandas.html#리샘플링",
    "title": "pandas Code! (1)",
    "section": "리샘플링",
    "text": "리샘플링\n판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n&lt;pandas.core.resample.DatetimeIndexResampler object at 0x00000186D389D4F0&gt;\n\n\n리샘플링 연산은 사실 지연된 연산입니다. (https://ko.wikipedia.org/wiki/%EB%8A%90%EA%B8%8B%ED%95%9C_%EA%B3%84%EC%82%B0%EB%B2%95) 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다:\n\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n\n\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n결과를 그래프로 출력해 보죠:\n\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").mean()\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "posts/Pandas.html#업샘플링과-보간",
    "href": "posts/Pandas.html#업샘플링과-보간",
    "title": "pandas Code! (1)",
    "section": "업샘플링과 보간",
    "text": "업샘플링과 보간\n다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다:\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head`는 상위 n 개의 값만 출력합니다\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\n한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: https://bskyvision.com/789\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Pandas.html#시간대",
    "href": "posts/Pandas.html#시간대",
    "title": "pandas Code! (1)",
    "section": "시간대",
    "text": "시간대\n기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다: https://www.timeanddate.com/time/map/\n\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\n모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다.\n다음처럼 파리 시간대로 바꿀 수 있습니다:\n\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nUTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다):\n\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\n이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다:\n\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n&lt;class 'pytz.exceptions.AmbiguousTimeError'&gt;\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\n다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다:\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tori-tours&logNo=221221361831\n\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "posts/Pandas.html#기간",
    "href": "posts/Pandas.html#기간",
    "title": "pandas Code! (1)",
    "section": "기간",
    "text": "기간\npd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠:\n\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nPeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다:\n\nquarters + 3\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nasfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠:\n\nquarters.asfreq(\"M\")\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\n\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\n기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다:\n\nquarters.asfreq(\"M\", how=\"start\")\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\n간격을 늘릴 수도 있습니다: pandas 공식 메뉴얼 참조: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n\nquarters.asfreq(\"A\")\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\n물론 PeriodIndex로 Series를 만들 수 있습니다:\n\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\n\nto_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다:\n\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\nlast_hours\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nto_peroid를 호출하면 다시 기간으로 돌아갑니다:\n\nlast_hours.to_period()\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다:\n\nmonths_2022 = pd.period_range(\"2022\", periods=12, freq=\"M\")\none_day_after_last_days = months_2022.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1)\nlast_bdays.to_period(\"H\") + 9\n\nPeriodIndex(['2022-01-31 09:00', '2022-02-28 09:00', '2022-03-31 09:00',\n             '2022-04-29 09:00', '2022-05-31 09:00', '2022-06-30 09:00',\n             '2022-07-29 09:00', '2022-08-31 09:00', '2022-09-30 09:00',\n             '2022-10-31 09:00', '2022-11-30 09:00', '2022-12-30 09:00'],\n            dtype='period[H]')"
  },
  {
    "objectID": "posts/Pandas.html#dataframe-만들기",
    "href": "posts/Pandas.html#dataframe-만들기",
    "title": "pandas Code! (1)",
    "section": "DataFrame 만들기",
    "text": "DataFrame 만들기\nSeries 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다:\n\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n\n\n\n\n\nweight\nbirthyear\nchildren\nhobby\n\n\n\n\nalice\n68\n1985\nNaN\nBiking\n\n\nbob\n83\n1984\n3.0\nDancing\n\n\ncharles\n112\n1992\n0.0\nNaN\n\n\n\n\n\n\n\n몇가지 알아 두어야 할 것은 다음과 같습니다:\n\nSeries는 인덱스를 기반으로 자동으로 정렬됩니다.\n누란된 값은 NaN으로 표현됩니다.\nSeries 이름은 무시됩니다(\"year\"란 이름은 삭제됩니다).\nDataFrame은 주피터 노트북에서 멋지게 출력됩니다!\n\n예상하는 방식으로 열을 참조할 수 있고 Series 객체가 반환됩니다:\n\npeople[\"birthyear\"]\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n동시에 여러 개의 열을 선택할 수 있습니다:\n\npeople[[\"birthyear\", \"hobby\"]]\n\n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\ncharles\n1992\nNaN\n\n\n\n\n\n\n\n열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면:\n\npeople_dict\n\n{'weight': alice       68\n bob         83\n charles    112\n dtype: int64,\n 'birthyear': bob        1984\n alice      1985\n charles    1992\n Name: year, dtype: int64,\n 'children': charles    0\n bob        3\n dtype: int64,\n 'hobby': alice     Biking\n bob      Dancing\n dtype: object}\n\n\n\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\n\n\nd2\n\n\n\n\n\n\n\n\nbirthyear\nweight\nheight\n\n\n\n\nbob\n1984.0\n83.0\nNaN\n\n\nalice\n1985.0\n68.0\nNaN\n\n\neugene\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nDataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다:\n\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3.0\nDancing\n83\n\n\ncharles\n1992\n0.0\nNaN\n112\n\n\n\n\n\n\n\n누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다:\ndtype = object는 문자열 데이터를 의미\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array\n\nmasked_array(\n  data=[[1985, nan, 'Biking', 68],\n        [1984, 3, 'Dancing', 83],\n        [1992, 0, nan, 112]],\n  mask=False,\n  fill_value='?',\n  dtype=object)\n\n\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3\nDancing\n83\n\n\ncharles\n1992\n0\nNaN\n112\n\n\n\n\n\n\n\nndarray 대신에 DataFrame 객체를 전달할 수도 있습니다:\n\nd3\n\n\n\n\n\n\n\n\nbirthyear\nchildren\nhobby\nweight\n\n\n\n\nalice\n1985\nNaN\nBiking\n68\n\n\nbob\n1984\n3\nDancing\n83\n\n\ncharles\n1992\n0\nNaN\n112\n\n\n\n\n\n\n\n\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n\n\n\n\n\n\n\nhobby\nchildren\n\n\n\n\nalice\nBiking\nNaN\n\n\nbob\nDancing\n3\n\n\n\n\n\n\n\n딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다:\n\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\n\npeople\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0"
  },
  {
    "objectID": "posts/Pandas.html#멀티-인덱싱",
    "href": "posts/Pandas.html#멀티-인덱싱",
    "title": "pandas Code! (1)",
    "section": "멀티 인덱싱",
    "text": "멀티 인덱싱\n모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면:\n\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n이제 \"public\" 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다:\n\nd5[\"public\"]\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\n\n\n\n\nParis\nalice\n1985\nBiking\n\n\nbob\n1984\nDancing\n\n\nLondon\ncharles\n1992\nNaN\n\n\n\n\n\n\n\n\nd5[\"public\", \"hobby\"]  # d5[\"public\"][\"hobby\"]와 같습니다.\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object\n\n\n\nd5[\"public\"]['hobby']\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: hobby, dtype: object"
  },
  {
    "objectID": "posts/Pandas.html#레벨-낮추기",
    "href": "posts/Pandas.html#레벨-낮추기",
    "title": "pandas Code! (1)",
    "section": "레벨 낮추기",
    "text": "레벨 낮추기\nd5를 다시 확인해 보죠:\n\nd5\n\n\n\n\n\n\n\n\n\npublic\nprivate\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다):\n\nd5.columns = d5.columns.droplevel(level = 0)\nd5\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n\nd6 = d5.copy()\nd6.index = d6.index.droplevel(level = 0)\nd6\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0"
  },
  {
    "objectID": "posts/Pandas.html#전치",
    "href": "posts/Pandas.html#전치",
    "title": "pandas Code! (1)",
    "section": "전치",
    "text": "전치\nT 속성을 사용해 열과 인덱스를 바꿀 수 있습니다:\n\nd5\n\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nParis\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\nLondon\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n\nd6 = d5.T\nd6\n\n\n\n\n\n\n\n\nParis\nLondon\n\n\n\nalice\nbob\ncharles\n\n\n\n\nbirthyear\n1985\n1984\n1992\n\n\nhobby\nBiking\nDancing\nNaN\n\n\nweight\n68\n83\n112\n\n\nchildren\nNaN\n3.0\n0.0"
  },
  {
    "objectID": "posts/Pandas.html#레벨-스택과-언스택",
    "href": "posts/Pandas.html#레벨-스택과-언스택",
    "title": "pandas Code! (1)",
    "section": "레벨 스택과 언스택",
    "text": "레벨 스택과 언스택\nstack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다:\n\nd6\n\n\n\n\n\n\n\n\nParis\nLondon\n\n\n\nalice\nbob\ncharles\n\n\n\n\nbirthyear\n1985\n1984\n1992\n\n\nhobby\nBiking\nDancing\nNaN\n\n\nweight\n68\n83\n112\n\n\nchildren\nNaN\n3.0\n0.0\n\n\n\n\n\n\n\n\nd7 = d6.stack()\nd7\n\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\n\nbirthyear\nalice\nNaN\n1985\n\n\nbob\nNaN\n1984\n\n\ncharles\n1992\nNaN\n\n\nhobby\nalice\nNaN\nBiking\n\n\nbob\nNaN\nDancing\n\n\nweight\nalice\nNaN\n68\n\n\nbob\nNaN\n83\n\n\ncharles\n112\nNaN\n\n\nchildren\nbob\nNaN\n3.0\n\n\ncharles\n0.0\nNaN\n\n\n\n\n\n\n\nNaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다).\nunstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다.\n\nd8 = d7.unstack()\nd8\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\nalice\nbob\ncharles\nalice\nbob\ncharles\n\n\n\n\nbirthyear\nNaN\nNaN\n1992\n1985\n1984\nNaN\n\n\nchildren\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\n\n\nhobby\nNaN\nNaN\nNaN\nBiking\nDancing\nNaN\n\n\nweight\nNaN\nNaN\n112\n68\n83\nNaN\n\n\n\n\n\n\n\nunstack을 다시 호출하면 Series 객체가 만들어 집니다:\n\nd9 = d8.unstack()\nd9\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nstack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다:\n\nd10 = d9.unstack(level = (0,1))\nd10\n\n\n\n\n\n\n\n\nLondon\nParis\n\n\n\nalice\nbob\ncharles\nalice\nbob\ncharles\n\n\n\n\nbirthyear\nNaN\nNaN\n1992\n1985\n1984\nNaN\n\n\nchildren\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\n\n\nhobby\nNaN\nNaN\nNaN\nBiking\nDancing\nNaN\n\n\nweight\nNaN\nNaN\n112\n68\n83\nNaN"
  },
  {
    "objectID": "posts/Pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "href": "posts/Pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "title": "pandas Code! (1)",
    "section": "대부분의 메서드는 수정된 복사본을 반환합니다",
    "text": "대부분의 메서드는 수정된 복사본을 반환합니다\n눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다.\nStack & Unstack + Pivot에 대한 설명 참고 https://pandas.pydata.org/docs/user_guide/reshaping.html\nData Reshaping!\n\nPivot\n\nimport pandas._testing as tm\n\ndef unpivot(frame):\n    N, K = frame.shape\n    data = {\n        \"value\": frame.to_numpy().ravel(\"F\"),\n        \"variable\": np.asarray(frame.columns).repeat(N),\n        \"date\": np.tile(np.asarray(frame.index), K),\n    }\n    return pd.DataFrame(data, columns=[\"date\", \"variable\", \"value\"])\n\ndf = unpivot(tm.makeTimeDataFrame(3))\n\n\ndf\n\n\n\n\n\n\n\n\ndate\nvariable\nvalue\n\n\n\n\n0\n2000-01-03\nA\n0.845824\n\n\n1\n2000-01-04\nA\n0.544290\n\n\n2\n2000-01-05\nA\n-0.513239\n\n\n3\n2000-01-03\nB\n1.459039\n\n\n4\n2000-01-04\nB\n0.716638\n\n\n5\n2000-01-05\nB\n0.475834\n\n\n6\n2000-01-03\nC\n1.761366\n\n\n7\n2000-01-04\nC\n-1.036727\n\n\n8\n2000-01-05\nC\n1.182743\n\n\n9\n2000-01-03\nD\n-0.596384\n\n\n10\n2000-01-04\nD\n-0.305634\n\n\n11\n2000-01-05\nD\n0.403161\n\n\n\n\n\n\n\nTo select out everything for variable A we could do:\n\nfiltered = df[df[\"variable\"] == \"A\"]\nfiltered\n\n\n\n\n\n\n\n\ndate\nvariable\nvalue\n\n\n\n\n0\n2000-01-03\nA\n0.845824\n\n\n1\n2000-01-04\nA\n0.544290\n\n\n2\n2000-01-05\nA\n-0.513239\n\n\n\n\n\n\n\nBut suppose we wish to do time series operations with the variables. A better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()):\n\npivoted = df.pivot(index=\"date\", columns=\"variable\", values=\"value\")\n\npivoted\n\n\n\n\n\n\n\nvariable\nA\nB\nC\nD\n\n\ndate\n\n\n\n\n\n\n\n\n2000-01-03\n0.845824\n1.459039\n1.761366\n-0.596384\n\n\n2000-01-04\n0.544290\n0.716638\n-1.036727\n-0.305634\n\n\n2000-01-05\n-0.513239\n0.475834\n1.182743\n0.403161\n\n\n\n\n\n\n\n\npivoted.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object', name='variable')\n\n\n\npivoted.index\n\nDatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05'], dtype='datetime64[ns]', name='date', freq=None)\n\n\nIf the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot(), then the resulting “pivoted” DataFrame will have hierarchical columns whose topmost level indicates the respective value column:\n\ndf[\"value2\"] = df[\"value\"] * 2\n\n\ndf\n\n\n\n\n\n\n\n\ndate\nvariable\nvalue\nvalue2\n\n\n\n\n0\n2000-01-03\nA\n0.845824\n1.691649\n\n\n1\n2000-01-04\nA\n0.544290\n1.088580\n\n\n2\n2000-01-05\nA\n-0.513239\n-1.026477\n\n\n3\n2000-01-03\nB\n1.459039\n2.918078\n\n\n4\n2000-01-04\nB\n0.716638\n1.433277\n\n\n5\n2000-01-05\nB\n0.475834\n0.951669\n\n\n6\n2000-01-03\nC\n1.761366\n3.522732\n\n\n7\n2000-01-04\nC\n-1.036727\n-2.073454\n\n\n8\n2000-01-05\nC\n1.182743\n2.365487\n\n\n9\n2000-01-03\nD\n-0.596384\n-1.192768\n\n\n10\n2000-01-04\nD\n-0.305634\n-0.611269\n\n\n11\n2000-01-05\nD\n0.403161\n0.806322\n\n\n\n\n\n\n\n\npivoted = df.pivot(index=\"date\", columns=\"variable\")\n\npivoted\n\n\n\n\n\n\n\n\nvalue\nvalue2\n\n\nvariable\nA\nB\nC\nD\nA\nB\nC\nD\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2000-01-03\n0.845824\n1.459039\n1.761366\n-0.596384\n1.691649\n2.918078\n3.522732\n-1.192768\n\n\n2000-01-04\n0.544290\n0.716638\n-1.036727\n-0.305634\n1.088580\n1.433277\n-2.073454\n-0.611269\n\n\n2000-01-05\n-0.513239\n0.475834\n1.182743\n0.403161\n-1.026477\n0.951669\n2.365487\n0.806322\n\n\n\n\n\n\n\n\npivoted['value']\n\n\n\n\n\n\n\nvariable\nA\nB\nC\nD\n\n\ndate\n\n\n\n\n\n\n\n\n2000-01-03\n0.845824\n1.459039\n1.761366\n-0.596384\n\n\n2000-01-04\n0.544290\n0.716638\n-1.036727\n-0.305634\n\n\n2000-01-05\n-0.513239\n0.475834\n1.182743\n0.403161"
  },
  {
    "objectID": "posts/Pandas.html#행-참조하기",
    "href": "posts/Pandas.html#행-참조하기",
    "title": "pandas Code! (1)",
    "section": "행 참조하기",
    "text": "행 참조하기\npeople DataFrame으로 돌아가 보죠:\n\npeople\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\nloc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다:\n\npeople['birthyear']\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople.loc[\"charles\"]\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\niloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다:\n\npeople.iloc[2]\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\n행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다:\n\npeople\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n\npeople.iloc[1:3]\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다:\n\npeople[np.array([True, False, True])]\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n불리언 표현식을 사용할 때 아주 유용합니다:\n\npeople[\"birthyear\"] &lt; 1990\n\nalice       True\nbob         True\ncharles    False\nName: birthyear, dtype: bool\n\n\n\npeople[people[\"birthyear\"] &lt; 1990]\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0"
  },
  {
    "objectID": "posts/Pandas.html#열-추가-삭제",
    "href": "posts/Pandas.html#열-추가-삭제",
    "title": "pandas Code! (1)",
    "section": "열 추가, 삭제",
    "text": "열 추가, 삭제\nDataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다:\n\npeople\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n\n\nbob\n1984\nDancing\n83\n3.0\n\n\ncharles\n1992\nNaN\n112\n0.0\n\n\n\n\n\n\n\n\npeople[\"age\"] = 2022 - people[\"birthyear\"]  # \"age\" 열을 추가합니다\npeople[\"over 30\"] = people[\"age\"] &gt; 30      # \"over 30\" 열을 추가합니다\n\npeople\n\n\n\n\n\n\n\n\nbirthyear\nhobby\nweight\nchildren\nage\nover 30\n\n\n\n\nalice\n1985\nBiking\n68\nNaN\n37\nTrue\n\n\nbob\n1984\nDancing\n83\n3.0\n38\nTrue\n\n\ncharles\n1992\nNaN\n112\n0.0\n30\nFalse\n\n\n\n\n\n\n\n\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\n\nbirthyears\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople\n\n\n\n\n\n\n\n\nhobby\nweight\nage\nover 30\n\n\n\n\nalice\nBiking\n68\n37\nTrue\n\n\nbob\nDancing\n83\n38\nTrue\n\n\ncharles\nNaN\n112\n30\nFalse\n\n\n\n\n\n\n\n\n# 딕셔너리도 유사함\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\n\n\nweights.pop(\"alice\")\n\n68\n\n\n\nweights\n\n{'bob': 83, 'colin': 86, 'darwin': 68}\n\n\n\ndel weights[\"bob\"]\n\n\nweights\n\n{'colin': 86, 'darwin': 68}\n\n\n새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다:\n\npeople.index\n\nIndex(['alice', 'bob', 'charles'], dtype='object')\n\n\n\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice 누락됨, eugene은 무시됨\npeople\n\n\n\n\n\n\n\n\nhobby\nweight\nage\nover 30\npets\n\n\n\n\nalice\nBiking\n68\n37\nTrue\nNaN\n\n\nbob\nDancing\n83\n38\nTrue\n0.0\n\n\ncharles\nNaN\n112\n30\nFalse\n5.0\n\n\n\n\n\n\n\n새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다:\n\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0"
  },
  {
    "objectID": "posts/Pandas.html#새로운-열-할당하기",
    "href": "posts/Pandas.html#새로운-열-할당하기",
    "title": "pandas Code! (1)",
    "section": "새로운 열 할당하기",
    "text": "새로운 열 할당하기\nassign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다:\n\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] &gt; 0\n)\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\nhas_pets\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\n\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\n\n\n\n\n\n\n\n\ndel people[\"body_mass_index\"]\n\n\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n\n\n\n\n\n\n\n할당문 안에서 만든 열은 접근할 수 없습니다:\n\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] &gt; 25\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n해결책은 두 개의 연속된 할당문으로 나누는 것입니다:\n\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] &gt; 25)\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\n임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다:\n\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] &gt; 25)\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다:\n\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] &gt; 25)\n)\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\n문제가 해결되었군요!\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\npeople[\"overweight\"] = people[\"body_mass_index\"]&gt;25\n\n\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue"
  },
  {
    "objectID": "posts/Pandas.html#표현식-평가",
    "href": "posts/Pandas.html#표현식-평가",
    "title": "pandas Code! (1)",
    "section": "표현식 평가",
    "text": "표현식 평가\n판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다.\n\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\n\n\"weight / (height/100) ** 2 &gt; 25\"\n\n'weight / (height/100) ** 2 &gt; 25'\n\n\n\npeople.eval(\"weight / (height/100) ** 2 &gt; 25\")\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\n할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다:\n\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\n'@'를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다:\n\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nTrue\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\n\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index &gt; @overweight_threshold\", inplace=True)\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue"
  },
  {
    "objectID": "posts/Pandas.html#dataframe-쿼리하기",
    "href": "posts/Pandas.html#dataframe-쿼리하기",
    "title": "pandas Code! (1)",
    "section": "DataFrame 쿼리하기",
    "text": "DataFrame 쿼리하기\nquery() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다:\n\npeople\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nFalse\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\n\n\n\n\n\n\npeople.query(\"age &gt; 30 and pets == 0\")\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nFalse\n\n\n\n\n\n\n\n\npeople[(people[\"age\"]&gt;30) & (people[\"pets\"] == 0)]\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nFalse\n\n\n\n\n\n\n\n\nmask = (people[\"age\"]&gt;30) & (people[\"pets\"] == 0)\n\n\npeople[mask]\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nFalse"
  },
  {
    "objectID": "posts/Pandas.html#dataframe-정렬",
    "href": "posts/Pandas.html#dataframe-정렬",
    "title": "pandas Code! (1)",
    "section": "DataFrame 정렬",
    "text": "DataFrame 정렬\nsort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠:\n\npeople.sort_index(ascending=False)\n\n\n\n\n\n\n\n\nhobby\nheight\nweight\nage\nover 30\npets\nbody_mass_index\noverweight\n\n\n\n\ncharles\nNaN\n185\n112\n30\nFalse\n5.0\n32.724617\nTrue\n\n\nbob\nDancing\n181\n83\n38\nTrue\n0.0\n25.335002\nFalse\n\n\nalice\nBiking\n172\n68\n37\nTrue\nNaN\n22.985398\nFalse\n\n\n\n\n\n\n\nsort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다:\n\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n\n\n\nage\nbody_mass_index\nheight\nhobby\nover 30\noverweight\npets\nweight\n\n\n\n\nalice\n37\n22.985398\n172\nBiking\nTrue\nFalse\nNaN\n68\n\n\nbob\n38\n25.335002\n181\nDancing\nTrue\nFalse\n0.0\n83\n\n\ncharles\n30\n32.724617\n185\nNaN\nFalse\nTrue\n5.0\n112\n\n\n\n\n\n\n\n레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다:\n\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n\n\n\n\n\nage\nbody_mass_index\nheight\nhobby\nover 30\noverweight\npets\nweight\n\n\n\n\ncharles\n30\n32.724617\n185\nNaN\nFalse\nTrue\n5.0\n112\n\n\nalice\n37\n22.985398\n172\nBiking\nTrue\nFalse\nNaN\n68\n\n\nbob\n38\n25.335002\n181\nDancing\nTrue\nFalse\n0.0\n83"
  },
  {
    "objectID": "posts/Pandas.html#dataframe-그래프-그리기",
    "href": "posts/Pandas.html#dataframe-그래프-그리기",
    "title": "pandas Code! (1)",
    "section": "DataFrame 그래프 그리기",
    "text": "DataFrame 그래프 그리기\nSeries와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다.\n예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다:\n\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다:\n\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\n\n선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요.\n\nHistogram\n\n\ndf4 = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000) + 1,\n        \"b\": np.random.randn(1000),\n        \"c\": np.random.randn(1000) - 1,\n    },\n    columns=[\"a\", \"b\", \"c\"],\n)\n\nplt.figure();\n\ndf4.plot.hist(alpha=0.5);\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\ndf4\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1.272288\n2.136562\n1.587199\n\n\n1\n0.171101\n0.210956\n-0.140191\n\n\n2\n3.041074\n-0.218132\n0.322108\n\n\n3\n0.784005\n-0.229859\n-1.531156\n\n\n4\n1.144468\n-0.162737\n0.058663\n\n\n...\n...\n...\n...\n\n\n995\n1.253080\n1.434197\n-1.731364\n\n\n996\n0.628578\n-0.157078\n-0.785003\n\n\n997\n0.954488\n-1.484172\n0.023209\n\n\n998\n1.671204\n-3.129176\n-0.654878\n\n\n999\n2.808931\n-0.677892\n-0.923582\n\n\n\n\n1000 rows × 3 columns\n\n\n\n\ndf4.plot(kind=\"hist\",alpha=0.5, x=\"a\")\nplt.show()\n\n\n\n\n\ndf4['a'].plot.hist()\nplt.show()\n\n\n\n\n\nBoxplot\n\n\ndf\n\n\n\n\n\n\n\n\ndate\nvariable\nvalue\nvalue2\n\n\n\n\n0\n2000-01-03\nA\n0.845824\n1.691649\n\n\n1\n2000-01-04\nA\n0.544290\n1.088580\n\n\n2\n2000-01-05\nA\n-0.513239\n-1.026477\n\n\n3\n2000-01-03\nB\n1.459039\n2.918078\n\n\n4\n2000-01-04\nB\n0.716638\n1.433277\n\n\n5\n2000-01-05\nB\n0.475834\n0.951669\n\n\n6\n2000-01-03\nC\n1.761366\n3.522732\n\n\n7\n2000-01-04\nC\n-1.036727\n-2.073454\n\n\n8\n2000-01-05\nC\n1.182743\n2.365487\n\n\n9\n2000-01-03\nD\n-0.596384\n-1.192768\n\n\n10\n2000-01-04\nD\n-0.305634\n-0.611269\n\n\n11\n2000-01-05\nD\n0.403161\n0.806322\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\ndf.plot.box();\n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\n\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\ndf\n\n\n\n\n\n\n\n\nCol1\nCol2\nX\n\n\n\n\n0\n0.845708\n0.181844\nA\n\n\n1\n0.158249\n0.145041\nA\n\n\n2\n0.381748\n0.687311\nA\n\n\n3\n0.531642\n0.428755\nA\n\n\n4\n0.966709\n0.799200\nA\n\n\n5\n0.132451\n0.208480\nB\n\n\n6\n0.322921\n0.735572\nB\n\n\n7\n0.714673\n0.837712\nB\n\n\n8\n0.965056\n0.441871\nB\n\n\n9\n0.107034\n0.387606\nB\n\n\n\n\n\n\n\n\nplt.figure();\n\nbp = df.boxplot(by=\"X\")\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n다음 pandas code(2)를 통해 더 배워봅시다."
  },
  {
    "objectID": "posts/seaborn_and_matplotlib.html",
    "href": "posts/seaborn_and_matplotlib.html",
    "title": "Quiz",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()\n\n\n\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n\n# We transform text categorical variables into numerical variables\npenguins[\"species_codes\"] = pd.Categorical(penguins[\"species\"]).codes\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\naxes[0].scatter(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", c=\"species_codes\", alpha=0.3)\n\n&lt;matplotlib.collections.PathCollection at 0x2237cb70460&gt;\n\n\n\n\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\n\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib 산점도\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )\n                  \n    # seaborn 추세선\n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn 산점도\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib 중심점\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n# plot 1: kde plot\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n## pd_cut을 사용한 방법\npenguins[\"bill_length_group\"] = pd.cut(penguins[\"bill_length_mm\"],\n                                        bins=[0,40,50,60],\n                                        labels=[\"0-40\",\"40-50\",\"50-60\"])\nsns.boxplot(x=\"bill_length_group\", y = \"bill_depth_mm\", data =penguins)\nsns.stripplot(x=\"bill_length_group\", y = \"bill_depth_mm\", data=penguins)\n\nsns.set_style(\"whitegrid\")\nplt.show()\n\n\n\n\n\ng = sns.FacetGrid(penguins, col = \"species\",col_wrap=4)\ng.map(sns.boxplot,\"bill_length_group\",\"bill_depth_mm\")\n\nC:\\Users\\heeyoung\\.conda\\envs\\quarto\\lib\\site-packages\\seaborn\\axisgrid.py:712: UserWarning: Using the boxplot function without specifying `order` is likely to produce an incorrect plot.\n  warnings.warn(warning)"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html",
    "href": "posts/week_1c_increase_loop_speed.html",
    "title": "For loop 속도 개선하기",
    "section": "",
    "text": "강의자료 출처 - https://blog.fearcat.in/a?ID=00900-6997c6fb-2680-4531-af1d-73eeccce74ef - https://aldente0630.github.io/data-science/2018/08/05/a-beginners-guide-to-optimizing-pandas-code-for-speed.html\nimport pandas as pd\nimport numpy as np\nfrom math import *"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#haversine-definition",
    "href": "posts/week_1c_increase_loop_speed.html#haversine-definition",
    "title": "For loop 속도 개선하기",
    "section": "Haversine definition",
    "text": "Haversine definition\n두 위치 사이의 거리를 계산하는 함수 - https://stricky.tistory.com/284\n\ndef haversine(lat1, lon1, lat2, lon2):\n    miles_constant = 3959 #단위가 miles\n    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1 \n    dlon = lon2 - lon1 \n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a)) \n    mi = miles_constant * c\n    return mi"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#task",
    "href": "posts/week_1c_increase_loop_speed.html#task",
    "title": "For loop 속도 개선하기",
    "section": "Task",
    "text": "Task\n어떤 위치, (40.671, -73.985)에서 df에 존재하는 모든 호텔까지의 거리를 구해봅시다"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#looping-haversine",
    "href": "posts/week_1c_increase_loop_speed.html#looping-haversine",
    "title": "For loop 속도 개선하기",
    "section": "Looping Haversine",
    "text": "Looping Haversine\n\ndef haversine_looping(df):\n    distance_list = [] # 빈 리스트를 생성\n    for i in range(0, len(df)):\n        d = haversine(40.671, -73.985, df.iloc[i]['latitude'], df.iloc[i]['longitude'])\n        distance_list.append(d)\n    return distance_list\n\n%%timeit은 Jupyter Notebook에서 사용되는 매직 명령어 중 하나로, 코드 실행 시간을 측정하는 도구입니다.\n%%timeit 매직 명령어를 사용하면 해당 셀의 코드를 여러 번 실행하여 실행 시간을 평균적으로 계산합니다. 이를 통해 코드의 실행 성능을 쉽게 측정하고 비교할 수 있습니다.\n\n%%timeit\n\n# Haversine 반복 함수 실행하기\ndf['distance'] = haversine_looping(df)\n\n198 ms ± 466 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ndf['distance'].describe()\n\ncount    1631.000000\nmean      111.318922\nstd       107.476086\nmin         0.163480\n25%         6.305530\n50%        71.070425\n75%       199.395866\nmax       314.936306\nName: distance, dtype: float64"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#iterrows-haversine",
    "href": "posts/week_1c_increase_loop_speed.html#iterrows-haversine",
    "title": "For loop 속도 개선하기",
    "section": "Iterrows Haversine",
    "text": "Iterrows Haversine\n반복문을 돌려야 할 때 iterrows() 메서드를 사용하는 건 행을 반복하기 위한 더 좋은 방법이다. iterrows()는 데이터 프레임의 행을 반복하며 행 자체를 포함하는 객체에 덧붙여 각 행의 색인을 반환하는 제너레이터다. iterrows()는 판다스 데이터 프레임과 함께 작동하게끔 최적화되어 있으며 표준 함수 대부분을 실행하는 데 가장 효율적인 방법은 아니지만(나중에 자세히 설명) 단순 반복보다는 상당히 개선되었다. 예제의 경우 iterrows()는 행을 수동으로 반복하는 것보다 거의 똑같은 문제를 약 4배 빠르게 해결한다.\n\n# Haversine applied on rows via iteration\nhaversine_series = []\nfor index, row in df.iloc[0:10].iterrows():\n    print(row['latitude'])\n\n42.68751\n42.68971\n42.7241\n42.65157\n42.68873\n42.72874\n42.68031\n42.65334\n42.72111\n42.67807\n\n\n\n%%timeit\n# Haversine applied on rows via iteration\nhaversine_series = []\nfor index, row in df.iterrows():\n    haversine_series.append(haversine(40.671, -73.985, row['latitude'], row['longitude'])) #function\ndf['distance'] = haversine_series\n\n80.6 ms ± 314 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nitertuples와 iterrows는 모두 Pandas 데이터프레임의 행을 순회(iterate)하는 메서드입니다. 그러나 itertuples는 iterrows보다 더욱 빠른 속도를 보이므로, 대체로 itertuples를 사용하는 것이 좋습니다.\n이유는 iterrows는 각 행(row)을 Series 객체로 반환하는 반면, itertuples는 각 행을 NamedTuple로 반환합니다. NamedTuple은 각 속성(attribute)에 이름이 지정되어 있기 때문에, Series보다 빠르게 데이터에 접근할 수 있습니다. 따라서 대용량의 데이터프레임을 다룰 때는 itertuples를 사용하는 것이 더욱 효율적입니다.\n\n%%timeit\nhaversine_series = []\nfor idx, lat, lon in df[['latitude','longitude']].itertuples(): # 필요한 컬럼만 뽑아서 itertuples을 진행 \n    haversine_series.append(haversine(40.671, -73.985, lat, lon))\n    \ndf['distance'] = haversine_series\n\n17 ms ± 63 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nimport pandas as pd\n\ndf2 = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\n\nfor row in df2.itertuples():\n    print(row)\n\nPandas(Index=0, A=1, B='a')\nPandas(Index=1, A=2, B='b')\nPandas(Index=2, A=3, B='c')\n\n\n\nimport pandas as pd\n\ndf2 = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\n\nfor index, a, b in df2.itertuples():\n    print(row)\n\nPandas(Index=2, A=3, B='c')\nPandas(Index=2, A=3, B='c')\nPandas(Index=2, A=3, B='c')"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#apply-haversine-on-rows",
    "href": "posts/week_1c_increase_loop_speed.html#apply-haversine-on-rows",
    "title": "For loop 속도 개선하기",
    "section": "Apply Haversine on rows",
    "text": "Apply Haversine on rows\niterrows()보다 더 좋은 옵션은 데이터 프레임의 특정 축(행 또는 열을 의미)을 따라 함수를 적용하는 apply() 메서드를 사용하는 것이다. apply()는 본질적으로 행을 반복하지만 Cython에서 이터레이터를 사용하는 것 같이 내부 최적화를 다양하게 활용하므로 iterrows()보다 훨씬 효율적이다.\n익명의 람다 함수를 사용하여 Haversine 함수를 각 행에 적용하며 각 행의 특정 셀을 함수 입력값으로 지정할 수 있다. 람다 함수는 판다스가 행(축 = 1)과 열(축 = 0) 중 어디에 함수를 적용할지 정할 수 있게 축 매개 변수를 마지막에 포함한다.\n\nTiming “apply”\n\n%%timeit \n\ndf['distance'] =\\\ndf.apply(lambda row: haversine(40.671, -73.985,\\\n                               row['latitude'], row['longitude']), axis=1)\n\n26.4 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#vectorized-implementation-of-haversine-applied-on-pandas-series",
    "href": "posts/week_1c_increase_loop_speed.html#vectorized-implementation-of-haversine-applied-on-pandas-series",
    "title": "For loop 속도 개선하기",
    "section": "Vectorized implementation of Haversine applied on Pandas series",
    "text": "Vectorized implementation of Haversine applied on Pandas series\n\nTiming vectorized implementation\n함수 수행의 반복량 줄이는 방법을 이해하기 위해 판다스의 기본 단위, 데이터 프레임과 시리즈가 모두 배열 기반임을 알아두자. 기본 단위의 내부 구조는 개별 값(스칼라라고 함)마다 순차적으로 작동하는 대신 전체 배열 위로 작동하도록 설계된 내장 판다스 함수를 위해 변환된다. 벡터화는 전체 배열 위로 작업을 실행하는 프로세스다.\n판다스는 수학 연산에서 집계 및 문자열 함수(사용 가능한 함수의 광범위한 목록은 판다스 문서에서 확인해라)에 이르기까지 다양한 벡터화 함수를 포함하고 있다. 내장 함수는 판다스 시리즈와 데이터 프레임에서 작동하게끔 최적화되어있다. 결과적으로 벡터화 판다스 함수를 사용하는 건 비슷한 목적을 위해 손수 반복시키는 방법보다 거의 항상 바람직하다.\n지금까지는 Haversine 함수에 스칼라를 전달했다. 그러나 “Haversine 함수 내에서 사용하는 모든 함수를 배열 위로 작동시킬 수 있다. 이렇게 하면 거리 함수를 매우 간단하게 벡터화할 수 있다.” 스칼라 값으로 각 위도, 경도를 전달하는 대신 전체 시리즈(열)를 전달한다. 이를 통해 판다스는 벡터화 함수에 적용 가능한 모든 최적화 옵션을 활용할 수 있고 특히 전체 배열에 대한 모든 계산을 동시에 수행하게 된다.\n\n%%timeit\n# Vectorized implementation of Haversine applied on Pandas series\ndf['distance'] = haversine(40.671, -73.985,\\\n                                   df['latitude'], df['longitude']) #벡터\n\n975 µs ± 1.38 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n함수 벡터화를 통해 apply() 메서드 대비 50배 이상 개선시켰고 iterrows() 대비 100배 이상 개선시켰다. 입력 유형 변경하는 것 외에 아무것도 하지 않아도 됐다!"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#vectorized-implementation-of-haversine-applied-on-numpy-arrays",
    "href": "posts/week_1c_increase_loop_speed.html#vectorized-implementation-of-haversine-applied-on-numpy-arrays",
    "title": "For loop 속도 개선하기",
    "section": "Vectorized implementation of Haversine applied on NumPy arrays",
    "text": "Vectorized implementation of Haversine applied on NumPy arrays\n이 지점에서 그만두어도 괜찮다. 판다스 시리즈를 사용해 벡터화하면 상시 계산을 위한 최적화 요구 사항의 거의 대부분을 만족시킬 수 있다. 그러나 속도가 최우선이라면 넘파이 파이썬 라이브러리 형식에 도움을 요청해볼 수 있다.\n넘파이 라이브러리는 “과학 계산을 위한 파이썬 기본 패키지”를 표방하며 내부가 최적화된, 사전 컴파일된 C 코드로 작업을 수행한다. 판다스와 마찬가지로 넘파이는 배열 객체(ndarrays라고 함) 상에서 작동한다. 그러나 색인, 데이터 유형 확인 등과 같이 판다스 시리즈 작업으로 인한 오버헤드가 많이 발생하지 않는다. 결과적으로 넘파이 배열에 대한 작업은 판다스 시리즈에 대한 작업보다 훨씬 빠르다.\n판다스 시리즈가 제공하는 추가 기능이 중요하지 않을 때 넘파이 배열을 판다스 시리즈 대신 사용할 수 있다. 예를 들어 Haversine 함수의 벡터화 구현은 실제로 위도 또는 경도 시리즈의 색인을 사용하지 않으므로 사용할 수 있는 색인이 없어도 함수가 중단되지 않는다. 이에 비해 색인으로 값을 참조해야 하는 데이터 프레임의 조인 같은 작업을 수행한다면 판다스 개체를 계속 사용하는 편이 낫다.\n위도와 경도 배열을 시리즈의 values 메서드를 단순 사용해서 판다스 시리즈에서 넘파이 배열로 변환한다. 시리즈의 벡터화와 마찬가지로 넘파이 배열을 함수에 직접 전달하면 판다스가 전체 벡터에 함수를 적용시킨다.\n\nTiming vectorized implementation\n\n# Vectorized implementation of Haversine applied on NumPy arrays\n%timeit df['distance'] = haversine(40.671, -73.985,\\\n                         df['latitude'].values, df['longitude'].values) #.values 빼고 실행이되지만 속도차이 있음\n\n114 µs ± 61 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n%%timeit\n# Convert pandas arrays to NumPy ndarrays\nnp_lat = df['latitude'].values\nnp_lon = df['longitude'].values\n\n5.24 µs ± 17.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)"
  },
  {
    "objectID": "posts/week_1c_increase_loop_speed.html#summary",
    "href": "posts/week_1c_increase_loop_speed.html#summary",
    "title": "For loop 속도 개선하기",
    "section": "Summary",
    "text": "Summary\n판다스 코드 최적화에 관해 몇 가지 기본적인 결론을 내릴 수 있다.\n1 . 반복을 피해라. 사용 사례 대부분의 경우 반복은 느리고 불필요하다.\n2 . 반복해야 하는 경우 반복 함수가 아닌 apply()를 사용해라.\n3 . 보통은 벡터화가 스칼라 연산보다 낫다. 대부분의 판다스 작업은 벡터화시킬 수 있다.\n4 . 넘파이 배열에서의 벡터 연산은 판다스 시리즈에서 수행하는 것보다 효율적이다.\n\n실습\n아래의 조건을 만족하는 호텔의 List를 출력해 봅시다.\n\n현재 나는 (“latitude”, “longitude”) = (40.671, -73.985) 위치에 있고, 숙박할 호텔을 찾고 있습니다.\n직선거리 기준으로 200마일 안쪽에 있었으면 좋겠습니다.\nstar_rating이 4 이상인 호텔을 찾고 있습니다.\n\n해당 조건을 만족하는 호텔들을 출력해봅시다\n\ndf\n\n\n\n\n\n\n\n\nean_hotel_id\nname\naddress1\ncity\nstate_province\npostal_code\nlatitude\nlongitude\nstar_rating\nhigh_rate\nlow_rate\ndistance\n\n\n\n\n0\n269955\nHilton Garden Inn Albany/SUNY Area\n1389 Washington Ave\nAlbany\nNY\n12206\n42.68751\n-73.81643\n3.0\n154.0272\n124.0216\n139.607190\n\n\n1\n113431\nCourtyard by Marriott Albany Thruway\n1455 Washington Avenue\nAlbany\nNY\n12206\n42.68971\n-73.82021\n3.0\n179.0100\n134.0000\n139.746898\n\n\n2\n108151\nRadisson Hotel Albany\n205 Wolf Rd\nAlbany\nNY\n12205\n42.72410\n-73.79822\n3.0\n134.1700\n84.1600\n142.191050\n\n\n3\n254756\nHilton Garden Inn Albany Medical Center\n62 New Scotland Ave\nAlbany\nNY\n12208\n42.65157\n-73.77638\n3.0\n308.2807\n228.4597\n137.275546\n\n\n4\n198232\nCrestHill Suites SUNY University Albany\n1415 Washington Avenue\nAlbany\nNY\n12206\n42.68873\n-73.81854\n3.0\n169.3900\n89.3900\n139.684583\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1626\n324259\nResidence Inn Yonkers Westchester County\n7 Executive Blvd\nYonkers\nNY\n10701\n40.97275\n-73.88075\n3.0\n269.3600\n169.3600\n21.551000\n\n\n1627\n107949\nRamada Inn Yonkers\n125 Tuckahoe Rd\nYonkers\nNY\n10710\n40.95466\n-73.86483\n2.5\n129.0000\n119.0000\n20.583072\n\n\n1628\n509723\nHyatt Place New York/Yonkers\n7000 Mall Walk\nYonkers\nNY\n10704\n40.92625\n-73.85438\n3.0\n249.3100\n199.3100\n18.914309\n\n\n1629\n621870\nHampton Inn & Suites Yonkers - Westchester\n555 Tuckahoe Rd\nYonkers\nNY\n10710\n40.95375\n-73.84935\n2.5\n189.1900\n134.1800\n20.785405\n\n\n1630\n683329\nCourtyard by Marriott Yonkers Westchester County\n5 Executive Boulevard\nYonkers\nNY\n10701\n40.97308\n-73.87955\n3.0\n0.0000\n0.0000\n21.588998\n\n\n\n\n1631 rows × 12 columns\n\n\n\n\ndf_1 = df[df[\"distance\"]&gt; 200]\ndf_1 = df[df[\"star_rating\"]&gt;=4]\n\n\ndf_1\n\n\n\n\n\n\n\n\nean_hotel_id\nname\naddress1\ncity\nstate_province\npostal_code\nlatitude\nlongitude\nstar_rating\nhigh_rate\nlow_rate\ndistance\n\n\n\n\n114\n482259\nTopping Rose House\nOne Bridgehampton - Sag Harbor Turnpike\nBridgehampton\nNY\n11932\n40.93780\n-72.30069\n4.5\n995.3900\n295.3900\n90.001264\n\n\n129\n339951\nSheraton Brooklyn New York Hotel\n228 Duffield Street\nBrooklyn\nNY\n11201\n40.69160\n-73.98436\n4.0\n409.1485\n259.0916\n1.423805\n\n\n134\n406834\nMcCarren Hotel & Pool\n160 N 12th St\nBrooklyn\nNY\n11249\n40.72100\n-73.95543\n4.0\n575.4100\n330.4100\n3.786288\n\n\n142\n403899\nThe Box House Hotel\n77 Box Street\nBrooklyn\nNY\n11222\n40.73749\n-73.95329\n4.0\n379.4200\n249.4200\n4.885345\n\n\n154\n136344\nNew York Marriott at the Brooklyn Bridge\n333 Adams St\nBrooklyn\nNY\n11201\n40.69365\n-73.98870\n4.0\n489.0700\n212.0400\n1.577023\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1592\n333528\nViana Hotel & Spa, BW Premier Collection\n3998 Brush Hollow Rd\nWestbury\nNY\n11590\n40.77629\n-73.55967\n4.0\n509.9900\n133.9500\n23.431169\n\n\n1600\n324657\nThe Ritz-Carlton New York, Westchester\n3 Renaissance Square\nWhite Plains\nNY\n10601\n41.03257\n-73.76664\n5.0\n479.4042\n349.2972\n27.466891\n\n\n1601\n514006\nFurnished Quarters Bank Street Commons\n15/25 Bank Street\nWhite Plains\nNY\n10606\n41.03050\n-73.77419\n4.0\n180.0000\n179.0000\n27.174550\n\n\n1602\n566642\nGlobal Luxury Apartments in White Plains\n15 Bank Street\nWhite Plains\nNY\n10606\n41.03150\n-73.77401\n4.0\n175.2200\n125.1500\n27.241501\n\n\n1603\n629966\nGlobal Luxury Suites at White Plains\n27 Barker Avenue\nWhite Plains\nNY\n10601\n41.03565\n-73.76925\n4.0\n190.0300\n189.0300\n27.604500\n\n\n\n\n258 rows × 12 columns"
  }
]